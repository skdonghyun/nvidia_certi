# Lesson 2 - Introduction to Artificial Intelligence
0:06
Welcome to the Introduction to Artificial Intelligence Unit. Today, we'll delve into the foundations of AI. We'll unravel the basics and provide you with a solid understanding of artificial intelligence, laying the groundwork for your journey into this exciting field. In this unit, we covered, an introduction to AI and its evolution through the years, typical steps of an AI workflow, a brief explanation of how deep learning works, ML and DL features and comparison, and challenges when deploying AI in production. By the end of this unit, you'll be able describe key milestones in the evolution of AI, visualize a typical AI workflow and the main steps in each phase. Describe in high level how neural networks work, identify common challenges enterprises face when adopting AI, and articulate the value of Nvidia end to end software stack for deploying AI solutions in production. Let's get started.
Play video starting at :1:13 and follow transcript1:13
AI is a broad field of study focused on using computers to do things that require human level intelligence. It has been around since the 1950s, used in games like tic tac toe and checkers, and inspiring scary Sci-Fi movies, but it was limited in practical applications. Machine Learning, or ML, came in the eighties as an approach to AI that uses statistics techniques to construct a model from observed data. It generally relies on human defined classifiers or feature extractors that can be as simple as a linear regression, or the slightly more complicated bag of words analysis technique that made email spam filters possible. This was very handy in the late 1980s when email spam started becoming an issue for many users. With the invention of smartphones, webcams and social media services, and all kinds of sensors that generate huge mountains of data, a new challenge presented itself, that of understanding and extracting insights from all this big data. Real breakthroughs with Deep Learning, or DL, came around 2010, largely due to advances in hardware, the availability of large datasets, and improvements in training algorithms. Which automated the creation of feature extractors using large amounts of data to train complex Deep Neural Networks, or DNNs. Within only one decade from the advancements brought by DNNs, we are now in a new era of Generative AI and large language models with systems that are surprisingly human like in their intelligence and capabilities. Applications such as chatbots, virtual assistants, content generation, translation services, and more are impacting industries and our daily lives. We will continue the discussion on generative AI in the next unit.
Play video starting at :3:9 and follow transcript3:09
An AI workflow often referred to as a machine learning workflow, or data science workflow. Is a sequence of tasks and processes that data scientists, machine learning engineers, and AI practitioners follow to develop, train, deploy, and maintain artificial intelligence models. These workflows help to ensure that AI projects are systematic, well documented, and effective. Let's consider a typical AI workflow broken down into four fundamental steps.
Play video starting at :3:44 and follow transcript3:44
The first step is data preparation, which involves collecting, cleaning, and pre processing raw data to make it suitable for training and evaluating artificial intelligence models. The size of a dataset used in model training can vary from small to very large datasets with billions of parameters. Ultimately, the quality, diversity, and relevance of the data are as important as the data set size. Once the data has been prepared accordingly, it is fed into a model. The model training step of an AI workflow involves using a machine learning or deep learning model to learn patterns and relationships within a labeled dataset. The model is trained over a set of data by using a mathematical algorithm to process and learn from the data. This is a critical part of the AI workflow. It involves teaching the AI model to recognize patterns and make predictions. Data scientists can iterate many times before producing a model that's ready for deployment. The training step is the most compute intensive phase of the AI workflow, usually benefiting the most from more hardware than the other steps, including inference. To make the training step more efficient while maintaining model accuracy, training is usually done with mixed precision data formats such as FP8 or FP16, among others. Model optimization is a crucial step in an AI workflow. It involves fine tuning and enhancing the performance of the AI model to make it more accurate, efficient, and suitable for its intended use case. It's an iterative process where adjustments are made based on evaluation results and the model is fine tuned until it meets the desired performance criteria for deployment. Once you've trained the model, it's ready to deploy into inference. It involves using a trained machine learning or deep learning model to make predictions, decisions, or generate outputs based on new unseen data. This step typically occurs after the model has been trained and validated and is ready to be deployed in a real world or production environment. Inference is often the core of AI applications, where the model's ability to provide meaningful and accurate outputs is essential for addressing real world challenges and achieving the application's objectives.
Play video starting at :6:8 and follow transcript6:08
Let's see a typical AI workflow example for deploying an image recognition solution alongside the tools that can be used in each step. ImageMe is a radiology clinic that provides services such as MRIs, X-rays, and CT scans to several doctor offices. They want to enhance their services by adding image recognition of fractures and tumors, helping doctors and their diagnostics. Sarah, an ML engineer, gathers historical datasets containing x rays, CT scans, and MRIs from hospital research institutes and their own inventory. For the data preparation step, she uses rapids, an open source suite of GPU accelerated Python libraries built on Nvidia AI, to perform analytics and to prepare data for machine learning. She leverages the Rapids accelerator for Apache Spark, a plugin software that automatically intercepts and accelerates operations that can be sped up with rapid software and GPUs, while allowing other operations to continue running on the CPU. Once the data prep is complete, Pytorch and Tensorflow are the GPU accelerated computational frameworks that can be used to train the model at scale. They are now integrated with Nvidia rapids to simplify enterprise AI development. Once the model training is complete, it can be optimized using Nvidia tensor RT, a deep learning inference optimizer to fine tune and improve the model's performance, making it ready to be deployed, executed, and scaled. Lastly, AI inference applies logical rules to the knowledge base to evaluate and analyze new information. She uses Nvidia Triton inference server as an open source software that standardizes AI model deployment, execution and takes care of all it and DevOps. Deployment aspects such as load balancing.
Play video starting at :8:6 and follow transcript8:06
We've discussed typical AI workflow steps and provided an example. Now let's shift our attention to the intricacies of deep learning.
Play video starting at :8:17 and follow transcript8:17
Jeffrey Hinton, the godfather of deep learning and AI, once said. I have always been convinced that the only way to get artificial intelligence to work is to do the computation in a way similar to the human brain. That is the goal I have been pursuing. We are making progress, though we still have lots to learn about how the brain actually works. Let's get a better understanding of what exactly Geoffrey meant.
Play video starting at :8:44 and follow transcript8:44
Earlier, we mentioned that deep learning harnesses deep neural networks, or DNNs, to achieve levels of accuracy that rival human capabilities. But have you ever wondered how these neural networks in AI are inspired by the human brain? Well, it all begins with a bit of neuroscience. Let's compare the works of a biological neuron to an artificial neuron.
Play video starting at :9:8 and follow transcript9:08
Artificial neural networks take a page from the human brain's playbook picture. In our brain, there are tiny components called neurons. Neurons are like tiny information messengers. They communicate through a series of events. First, dendrites, which act as the receiving antennas of neurons, pick up signals from neighboring neurons terminal buttons. These signals are then sent to the cell nucleus for some processing magic. After that, the electrical impulse zips along a longer branch, called the axon, making its way to the synapse. The synapse acts as a bridge, passing the impulse to the dendrites of another neuron. It's like a relay race of information, creating a complex neural network in the human brain. As you can tell from the on screen animation, artificial neurons are fundamentally inspired by the workings of biological neurons. So what is the deep learning workflow? Consider an application that automatically identifies various types of animals, in other words, a classification task. The first step is to assemble a collection of representative examples to be used as a training data set, which will serve as the experience from which a neural network will learn. As we just learned, neural networks are algorithms that draw inspiration from the human brain in understanding complex patterns.
Play video starting at :10:35 and follow transcript10:35
If the classification is only cats versus dogs, then only cat and dog images are needed in the training data set. In this case, several thousand images will be needed, each with a label indicating whether it is a cat image or a dog image. To ensure the training dataset is representative of all the pictures of cats and dogs that exist in the world. It must include a wide range of species, poses, and environments in which dogs and cats may be observed.
Play video starting at :11:5 and follow transcript11:05
The next component that is needed is a deep neural network model. Typically, this will be an untrained neural network designed to perform a general task like detection, classification, or segmentation on a specific type of input data like images, text, audio, or video.
Play video starting at :11:24 and follow transcript11:24
Shown here is a simple model of an untrained neural network. At the top of the model there is a row or layer that has five input nodes, and at the bottom there is a layer that has two output nodes. Between the input layer and the output layer are a few hidden layers with several nodes each. The interconnecting lines show which nodes in the input layer share their results with nodes in the first hidden layer, and so on all the way down to the output layer. Nodes may be referenced as artificial neurons or perceptrons, since their simple behavior is inspired by the neurons in the human brain.
Play video starting at :12:3 and follow transcript12:03
A typical deep neural network model would have many hidden layers between the input layer and the output layer, which is why it is called deep. We use a simplified representation on this slide. For brevity, the design of the neural network model is what makes it suitable for a particular task. For example, image classification models are very different from speech recognition models. The differences can include the number of layers, the number of nodes in each layer, the algorithms performed in each node, and the connections between the nodes. There are readily available deep neural network models for image classification, object recognition, image segmentation, and several other tasks. But it is often necessary to modify these models to achieve high levels of accuracy for a particular dataset.
Play video starting at :12:55 and follow transcript12:55
For the image classification task to distinguish images of cats versus dogs, a convolutional neural network such as Alexnet would probably be used. AlexNet is comprised of nodes that implement simple generalized algorithms. Using these simple generalized algorithms is a key difference and advantage for deep learning versus earlier approaches to machine learning. Which required many custom data specific feature extraction algorithms to be developed by specialists for each dataset and task.
Play video starting at :13:29 and follow transcript13:29
Once a training data set has been assembled and a neural network model selected, a deep learning framework is used to feed the training data set through the neural network. For each image that is processed through the neural network, each node in the output layer reports a number that indicates how confident it is that the image is a dog or a cat. In this case, there are only two options, so the model needs just two nodes in the output layer, one for dogs and one for cats. When these final outputs are sorted in a most confident to least confident manner, the result is called a confidence vector. The deep learning framework then looks at the label for the image to determine whether the neural network guessed or inferred the correct answer. If it inferred correctly, the framework strengthened the weights of the connections that contributed to getting the correct answer, and vice versa. If the neural network inferred the incorrect result, the framework reduces the weights of the connections that contributed to getting the wrong answer.
Play video starting at :14:29 and follow transcript14:29
After processing the entire training dataset once, the neural network will generally have enough experience to infer the correct answer a little more than half of the time. Slightly better than a random coin toss. It'll require several additional rounds to achieve higher levels of accuracy.
Play video starting at :14:50 and follow transcript14:50
Now that the model has been trained on a large representative data set, it has become better at distinguishing between cats and dogs. But if it were shown a picture of a raccoon, it would likely assign comparable confidence scores to both the dog and cat, as it wouldn't be certain about identifying either one.
Play video starting at :15:9 and follow transcript15:09
If it was necessary to classify raccoons as well as dogs and cats, the design topology of the model would need to be modified to add a third node to the output layer. The training data set would be expanded to include thousands of representative images of raccoons and use the deep learning framework to retrain the model.
Play video starting at :15:32 and follow transcript15:32
Once the model has been trained, much of the generalized flexibility that was necessary during the training process is no longer needed. So it is possible to optimize the model for significantly faster runtime performance.
Play video starting at :15:44 and follow transcript15:44
Common optimizations include fusing layers to reduce memory and communication overhead, pruning nodes that do not contribute significantly to the results, and other techniques. The fully trained and optimized model is then ready to be integrated. Graded into an application that will feed it new data, in this case, images of cats and dogs that it hasn't seen before. As a result, it will be able to quickly and accurately infer the correct answer based on its training.
Play video starting at :16:15 and follow transcript16:15
Let's summarize the key differences in the realm of AI we've covered till now. When most technology companies talk about doing AI, they're talking about using machines to mimic human abilities to learn, analyze, and predict. Machine learning achieves that by using large datasets and sophisticated statistical methods to train a model to predict outcomes from new incoming information.
Play video starting at :16:39 and follow transcript16:39
One of the most popular machine learning techniques used these days is deep learning. Deep learning uses artificial neural networks to learn from vast amounts of data to solve AI problems and really shines for use cases involving vision and speech. Generative AI is a type of artificial intelligence that uses machine learning algorithms to learn patterns and trends from the training data, using neural networks to create new content that mimics human generated content.
Play video starting at :17:10 and follow transcript17:10
Now that we've gained a deeper understanding of AI and the intricacies of deep learning workflows, let's turn our attention to the individuals and organizations who are actively employing AI. We'll also explore the challenges they encounter while trying to harness the capabilities of AI.
Play video starting at :17:27 and follow transcript17:27
In an AI data center, various stakeholders play crucial roles in the planning, development, and operation of AI infrastructure and applications. AI practitioners create applications that extract meaningful data from large data sets using machine learning. They desire an agile, cloud native platform that can quickly evolve to support the latest in AI with accuracy and performance that can accelerate time to deployment. For enterprise IT, who manages the company's infrastructure, data and application lifecycle, AI is still an emerging workload and rapidly evolving. Managing rapidly changing applications and often open source platforms and tools can be a challenge for an IT department that is most likely also dealing with older infrastructure and technical debt. For example, they want to ensure the data centers infrastructure meets AI workloads demands with an optimized platform to bring AI into production. Line of business managers want to see more models deployed in production sooner by ensuring the efficient utilization of the investments in infrastructure and platforms. Leaders are constantly looking for the quickest way to excel, return on investment, and provide data driven results from their investment in AI infrastructure.
Play video starting at :18:47 and follow transcript18:47
The benefits of AI are massive, but fully realizing those benefits requires a comprehensive solution. Along with many of these benefits come certain challenges, which are essential to consider when adopting AI.
Play video starting at :19:2 and follow transcript19:02
Exploding model sizes and complexity, state of the art AI models continue to rapidly evolve and expand in size, complexity, and diversity. The rapid growth of AI models demands extensive computational resources and energy, potentially limiting affordability and sustainability, and posing accessibility challenges for smaller organizations. The versatility required to deliver rich experience, AI enabled applications like product recommendations, voice assistance. And contact center automation may require multiple different powerful models to be deployed within the same application in order to deliver a fantastic user experience. Performance and scalability, training these AI models and customizing them for your unique application is an intense, complex, iterative process. End to end performance, considering both the individual steps and each overall iteration is critical for accelerating toward a solution. Taking AI to production requires tools to support the end to end AI lifecycle, compute infrastructure. And a robust support model to ensure all key stakeholders, the data scientists, engineers, developers, and operators, are able to meet their unique goals.
Play video starting at :20:22 and follow transcript20:22
NVIDIA contributes to addressing these challenges by providing AI practitioners with top tier development tools, frameworks, and pre trained models. Additionally, the platform offers reliable management and orchestration solutions for IT professionals, guaranteeing performance, high availability, and security. The NVIDIA AI software stack enables the full AI pipeline from data prep for model training through inferencing and ultimately scaling. It accelerates time to production with AI workflows and pre trained models for specific business outcomes such as intelligent virtual assistants, digital fingerprinting for real time cybersecurity, threat detection. And recommender systems for online retail to help understand consumer behaviors and increase customer engagement.
Play video starting at :21:9 and follow transcript21:09
Finally, your AI solution is optimized and certified to deploy everywhere from public cloud, to data centers, to the edge devices. This provides the flexibility and reduces the risk of moving from pilot to production caused by infrastructure and architectural differences between environments.
Play video starting at :21:30 and follow transcript21:30
Well done for making it to the end of this unit. Now you should be able to describe key milestones in the evolution of AI. Visualize a typical AI workflow and the main steps in each phase. Describe in high level how neural networks work. Identify common challenges enterprises face when adopting AI. Articulate value of NVIDIA end-to-end software stack for deploying AI solutions in production. Continue the journey by taking the next unit, Generative AI overview.