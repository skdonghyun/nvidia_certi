# Lesson 10 - Energy Efficient Computing

0:05
In this unit, we will be focusing on energy-efficient computing and your data center. Let's cover the learning objectives for this unit. By the end of this unit, you should be able to thoughtfully articulate the steps in the planning and deployment of a data center, including the equipment that will be installed in the data center. First, we'll go over power consumption and cooling considerations. After that, we'll discuss how Nvidia's technology is designed and optimized for efficiency. Next, you'll learn how Nvidia mitigates negative impact in the data center with efficient cooling systems. Finally, you'll see how data center colocation impacts and improves efficiency. Nvidia strives for low environmental impact by ensuring GPUs consume fewer resources and run as efficiently as possible. Let's start by covering data center considerations and Nvidia GPU architectures. Planning a data center deployment requires a balance between all five of the process domains: data center operations, IT operations, NOC support, the application owner, network operations. These five domains must continuously be coordinated and balanced to ensure a successful deployment. At a high level, data center resources can be categorized as power, cooling, and space. Given that data centers have finite resources, a change in one of these resources impacts the other two. This drives the need to optimize resource utilization for efficiency. The graph above illustrates the recent explosion of energy demand in data centers due to the massive data loads, complex data models, and the extreme processing power required to run today's applications and tools. As computing is sophisticated and realize new possibilities in today's applications, especially AI tools and applications, the need for data center resources such as power and space has increased substantially. Accelerated computing with GPU technology is optimized efficiency in the data center. This is because individual GPUs handle large scale compute-intensive functions with less technology and require less space. If one were to compare workloads on either a CPU or a GPU, while the GPU requires more power consumption, the amount of time the workload runs is significantly reduced. The increase in power consumed at a given time is offset by the fact that the workload runs so quickly, thus using less energy over time. Another benefit is that multi-instance GPUs allow users to partition the GPU, and each partition can have their workloads run simultaneously while not increasing the power consumption of the GPU. Processing capabilities have grown exponentially in the past decade, fueled largely by supercomputers, data centers, and cloud computing. A data center within Nvidia GPUs requires a fraction of the space and energy. A hyperscale data center with Nvidia GPUs takes up only 1/47th of the rack space of the CPU-based systems that it replaces and runs at 93% lower energy cost for AI models. Software can significantly improve the energy efficiency of AI workloads. We're continuously optimizing our CUDA-X libraries and GPU-accelerated applications, so it's not unusual for users to see an x-factor performance gain on the same GPU architecture. AI workloads on Nvidia Ampere architecture improved by 2.5x over the past two years. We offer the latest versions of AI and HPC software from the Nvidia GPU Cloud or NGC portal to help users run applications with better performance on their supercomputer in the data center or in the cloud. We estimate an energy savings of 20% on NGC workloads because of users implementing performance suggestions. As supercomputers take on more workloads, CPUs are stretched to support a growing number of communication tasks needed to operate large and complex systems. Data processing units or DPUs, which move data around the data center, alleviate 30% or more of this stress by offloading some of these processes from the CPU. Some workloads achieve more than 50x performance improvement, allowing fewer servers to be deployed and reducing power of a modest data center by four megawatts. The zero-trust protection platform enabled by Nvidia DPUs brings a new level of security to data centers at speeds up to 600 times faster than servers without Nvidia accelerations, further reducing the amount of infrastructure and power it would require. Built for AI, the Nvidia Spectrum-4 ethernet switch enables extreme networking performance and robust security with 40% lower power consumption compared to the previous generation. Adequate cooling is required to optimize supercomputer performance. We deploy state-of-the-art technology designed for Nvidia server products using computational fluid dynamics models to enhance cooling for data center designs and server rack deployments. We use physics-informed neural networks or PINNs available in Nvidia modulus to design heat sinks for our DGX systems. Cooling solutions are closely coupled with server racks to localize and optimize heat transfer. We share our data center best practices with customers and partners to help optimize their deployments. In partnership with leading storage and networking technology providers, we offer a portfolio of reference architectures for optimal and efficient deployment of our DGX server products, and we make these publicly available on our corporate website. One of the super powers of accelerated computing is energy efficiency in terms of application throughput per kilowatt hour. This is a simple study of several common HPC applications and their performance on the HGX H100 4x GPU system compared to a dual socket Sapphire Rapids' 84 ADC 52 cores per socket system. For a balanced amount of runtime for each application, there is a geo mean performance advantage to the GPU systems of 22.9X, so it would take 23 times the number of CPU servers to achieve the same throughput. We assumed that both the GPU and CPU systems ran at TDP, thermal design power, to estimate that a 50-node HGX supercomputer would use 2.6 gigawatt hours annually, and the CPU system with 1,150 servers would require 12.1 gigawatt hours. Clearly, the accelerated platform has a big energy efficiency advantage. When looking at data requirements and compute capabilities of the A100 versus the H100 for deploying AI workloads, including HDX H100 in the data center is an optimal solution. The H100 requires fewer servers in the data center while still managing the same workload as significantly more A100s. More specifically, 64 H100 clocks in at a third of the TCO using a fifth of the server nodes and is 3.5 times more energy efficient. The DGX H100 system is the compute building block of the DGX SuperPOD, and it offers benefits for extreme performance for AI workloads. To deliver such performance, it has specific power, environmental, and cooling requirements that must be met for optimal performance and operation. An important characteristic is that the system is air cooled, and it requires that air temperature remains between 5-30 degrees Celsius or between 41-86 degrees Fahrenheit. When a group of these powerful systems are brought online, even more challenges arise. Now that we've discussed Nvidia's approaches to reduce rack space and power consumption, let's talk about data center cooling and how Nvidia optimizes cooling to improve efficiency. Let's look at the HGX H100 as well as its associated PCIe. Liquid-cooled GPUs require less power, are smaller, and as a result require less rack space to meet Nvidia's efficiency targets with a PUE of less than 1.2. GPUs are the hottest thing in computing. 99.99% of the power used by the chip is converted to heat. As CPUs grow in power, their heat output has increased from eight watts to 150 watts per chip. Consequently, the heat output of CPU racks can range from four kilowatts to 12 kilowatts. In comparison, GPUs run at 250-500 watts per chip, dramatically increasing heat. The heat output of a GPU rack can range from 12 kilowatts to 45 kilowatts. Next-generation racks could increase to 60 kilowatts, while future racks can reach 100-120 kilowatts. These are the current options for cooling GPU chips. The first option is cold air. Cooling via cold air flow is inexpensive, but reaches its limit when cooling 30 kilowatts per rack. The second option is to use water-cooled heat exchangers. In this case, heat rejection is more efficient but more expensive. Because of this, water-cooled heat exchangers are now the accepted standard for high density cooling. They can serve between 20 kilowatts and 60 kilowatts per rack. Some manufacturers claim that it can serve over 100 kilowatts per rack.
Play video starting at :9:34 and follow transcript9:34
Let's consider direct air-cooled systems. The steps involved in this process are as follows. Computer room air handling units or CRAHs use chilled water circulated through coils with large fans to blow the hot air over the coils to absorb the heat. The fans in the CRAH units pressurize the cold air under the server room floor to distribute the air to all the systems and racks around the room. Floor grates inserted in the raised floor inside the contained aisles allow the cold air to circulate in front of the racks on both sides of the aisle. The fans of the systems being cooled draw in the cold air from the aisle, and the heat from the chips raise the air temperature before the system fans exhaust the hot air out from the back of the system. The hot air returns to the CRAH units in the ceiling and is then drawn into the CRAH by its fans, where the heat is transferred to the chilled water inside the coil. The hot aisle/cold aisle layout optimizes the airflow and cooling efficiency of a data center. Installing blanking panels wherever possible is an important consideration to ensure proper air flow throughout the data center rack. Let's consider the second cooling option, heat rejection to air or water. The characteristics of this option include rear doors with chilled water coils. Coils are only six inches from servers. The chilled water captures heat from the servers. Heat is transferred to the exterior to be dissipated. Rear door heat exchangers are often used on solid or slab floor facilities, while cold aisle containment air cooled systems are often deployed in raised floor facilities. Data center power provisioning must be completed prior to connecting power to the in rack power distribution units or PDUs and system deployment. Nvidia recommends that each component be supplied with redundant power sources to increase system reliability. AC power redundancy should be validated at each rack. An electrician or facility representative must verify that the AC voltage and total kilowatts supplied is within specifications at each of the floor-mounted PDUs and individual circuits, that is power drops that feed the racks. The equipment served by each circuit breaker within the PDU should be clearly labeled. Let's take a few minutes to talk about the benefits of Nvidia's DGX data center colocation program in saving data center resources and improving overall efficiency. Businesses are becoming increasingly aware of the advantages of accelerated computing with GPUs. Nvidia and its partners are at the forefront of the adoption of GPU computing in the data center. With DGX-based systems offering unprecedented compute density designed to handle the world's most complex AI challenges, the systems have been rapidly adopted by a wide range of organizations across dozens of countries. Internet service companies, healthcare facilities, government labs, financial institutions, oil and gas businesses, and more have all benefited from building and deploying their own DGX system-based AI data centers. However, some businesses don't have the modern data center facilities that can support accelerated computing operations. A single DGX H100 system draws a maximum of 10.2 kilowatts. Nvidia's current DGX SuperPOD reference architecture exceeds 40 kilowatts per rack. Many enterprises cannot support more than 8-15 kilowatts per rack in their existing data centers and many even less. With the Nvidia DGX-ready data center program built on Nvidia DGX systems and delivered by Nvidia partners, you can accelerate your AI mission today. The newly enhanced program offers a pairing function that connects you with the best partner for your needs and is now available in Asia, Australia, Europe, North America, and South America with more markets coming soon. Also, select partners are providing a broader range of services, including test drive and GPU as a service offerings. Through colocation, customers can avoid the challenges of facilities planning or the high costs and latency of the public cloud and instead focus on gaining insights from data while innovating. With this program, businesses can deploy in Nvidia DGX systems and recently announced DGX reference architecture solutions from DDN, IBM Storage, NetApp, Pure Storage, and Dell EMC with speed and simplicity at an affordable OpEx model. Nvidia continues to strive for a net zero data center with improvements across the GPU hardware from generation to generation as well as across networking equipment. From Ampere to Hopper, there was significant improvement, and combined with the ability to run AI workloads faster and more efficiently, the amount of time a GPU is in use is reduced. For example, the Nvidia Ampere GPU architecture introduced TF32 math mode for accelerating single precision DL training, achieving the same accuracy as FP32 training while providing it out of the box 10x faster tensor math than single precision math on Volta GPUs. Let's wrap up this unit by briefly discussing Nvidia's goals for net zero by deploying the most efficient data center servers as possible before summarizing what you learned. Now that you've completed this unit, you should be able to articulate the design and planning of a data center and see how space, power, and cooling consideration affect the plans, discuss how Nvidia's methods and servers optimize energy efficiency in data centers, describe cooling architecture of GPUs to improve efficiency, understand how colocation improves efficiency. Don't stop here. Continue the learning journey with Unit 11, AI reference architectures. See you there.