# Lesson 5 - AI Software Ecosystem
0:06
Welcome, in this unit, we'll cover the software ecosystem that has allowed developers to make use of GPU computing for data science.
Play video starting at ::17 and follow transcript0:17
We'll start with a brief overview of VGPU as a foundational technology. From there, we'll move into what frameworks are and their benefits with AI. Well also provide an overview of the Nvidia software stack and CUDA X AI software acceleration libraries. Later, well move on to Nvidia containerized software catalog known as NGC and discuss how Nvidia is extending AI to every enterprise using virtualization with Nvidia AI enterprise software suite. By the end of this unit, you'll be able to understand virtual GPU as a foundational technology upon which the AI ecosystem sits. Briefly describe the deep learning stack and CUDA, define the steps that make up the AI workflow. Identify the various types of workflows from open source, third party vendors as well as those provided by Nvidia. See what makes up NGC and the enterprise catalog and discuss their benefits. Walk through and describe the benefits and features of Nvidia AI enterprise and Nvidia's provided AI workflows. Let's get started. Before we get into AI frameworks and the way Nvidia provides and supports these frameworks, let's take a few minutes to briefly cover VGPU as a foundational technology. The workplace is experiencing a pandemic disruption that is changing the form and perspective about how we work. The adoption of digital technologies has helped organizations respond to the unprecedented challenges and increasingly make a mobile workforce more prevalent. By 2030, end user computing is expected to grow to $20 billion and 40% of storage and compute, shifting towards service based models. However, to build an enhanced digital workspace for the post pandemic recovery and beyond, we must move beyond defensive short team models. And focus on sustainable, resilient operating methods. Improved user experience paired with security stands at the forefront of the corporate agenda. In fact, 53% of IT executives report their companies are increasing investment in digital transformation, while 49% are looking to improve efficiencies. This is where Nvidia virtual GPU technology comes into play, allowing IT to deliver graphics rich virtual experiences across their user base. Whether deploying office productivity applications for knowledge workers. Or providing engineers and designers with high performance virtual workstations to access professional design and visualization applications. IT can deliver an appealing user experience and maintain the productivity and efficiency of their users. Application and desktop virtualization solutions have been around for a long time, but their number one point of failure tends to be user experience. The reason is very simple, when applications and desktops were first virtualized, GPU's were not a part of the mix. This meant that all of the capture, encode and rendering that was traditionally done on a GPU in a physical device was being handled by the CPU in the host. Enter Nvidia's virtual GPU or VGPU solution. It enables IT to virtualize a GPU and share it across multiple virtual machines or VMs. This not only improves performance for existing VDI environments, but it also opens up a whole new set of use cases that can leverage this technology. With our portfolio of virtual GPU solutions, we enable accelerated productivity across a wide range of users and applications. Knowledge workers benefit from an improved experience with office applications, browsers, high definition video, including video conferencing like Zoom, WebEx, and Skype. For creative and technical professionals, Nvidia enables virtual access to the professional applications typically run on physical workstations. Including CAD applications or design applications such as Revit and Maya. It enables GIS apps like Esri, ArcGIS Pro, oil and gas apps like petrol, financial services like Bloomberg, healthcare apps like Epic. Or manufacturing apps like CATIA, Siemens NX and SolidWorks, to name a few. Our virtual software is available for on prem data centers and also in the cloud. Nvidia Virtual PC, VPC and virtual apps software for the knowledge and business workers, Nvidia RTX virtual workstation for creative and technical professionals. Such as engineers, architects, and designers, we have a series of courses to walk you through each software offering. Please review the virtualization sales curriculum for more detailed information.
Play video starting at :5:12 and follow transcript5:12
Let's review how Nvidia virtual GPU software enables multiple virtual machines to have direct access to a single physical GPU. While using the same Nvidia drivers that our customers deploy on non virtualized operating systems. On the left hand side, we have a standard VMware ESXi host. VMware has done a great job over the years virtualizing CPU workloads. However, certain tasks are more efficiently handled by dedicated hardware such as GPU's, which offer enhanced graphics and accelerated computing capabilities. On the right side, from the bottom up, we have a server with a GPU running the ESXi hypervisor. When the Nvidia VGPU manager software, or VIB, is installed on the host server, we're able to assign VGPU profiles to individual VMs. Nvidia branded drivers are then installed into the guest OS, providing for a high end user experience. This software enables multiple VMs to share a single GPU, or if there are multiple GPU's in the server, they can be aggregated so that a single VM can access multiple GPUs. This GPU enabled environment provides for unprecedented performance. While enabling support for more users on a server, because work that was done by the CPU can now be offloaded to the GPU. Most people understand the benefits of GPU virtualization, the ability to divide up GPU resources and share it across multiple virtual machines to deliver the best possible performance. But there are many other benefits delivered by Nvidia virtual GPU software, included in the Nvidia AI enterprise suite, which go beyond just GPU sharing. With Nvidia VGPU software, IT can deliver bare metal performance for compute workloads with minimal overhead, running virtualized. Integrations with partners like VMware provide it a complete lifecycle approach to operational management, from infrastructure rightsizing to proactive management and issue remediation. These integrations allow IT to use the same familiar management tools from hypervisor and leading monitoring software vendors for deep insights into GPU usage. Nvidia VGPU supports live migration of accelerated workloads without interruption to end users. This allows for business continuity and workload balancing. The ability to flexibly allocate GPU resources means that IT can better utilize the resources in their data center. Since virtualization enables all data to remain securely in the data center, the solution helps to ensure infrastructure and data security. Lets now explore deep learning, we'll start with a brief review of what it is, then walk through an AI workflow. From there, we'll talk We'll talk about the AI software stack and CUDAX.
Play video starting at :8:6 and follow transcript8:06
Deep learning is a subclass of machine learning. It uses neural networks to train a model using very large datasets in the range of terabytes or more of data. Neural networks are algorithms that mimic the human brain in understanding complex patterns. Labeled data is a set of data with labels that help the neural network learn. In the example here, the labels are the objects in the images, cars, and trucks. The errors that the classifier makes on the training data are used to incrementally improve the network structure. Once the neural network-based model is trained, it can make predictions on new images. Once trained, the network and classifier are deployed against previously unseen data, which is not labeled. If the training was done correctly, the network will be able to apply its feature representation to correctly classify similar classes in different situations.
Play video starting at :9:4 and follow transcript9:04
To understand the AI ecosystem, you have to start with the workflow. The first step is the process of preparing raw data and making it suitable for the machine learning model. Examples of tools for this are NVIDIA rapids and the NVIDIA rapids accelerator for Apache Spark. Once the data is processed, we move on to the training phase. This is where we teach the model to interpret data. Examples of tools for this are PyTorch, the NVIDIA toolkit, and TensorFlow. Next, we refine the data through optimization. An example tool for this is TensorRT. Finally, we deploy the model, making it available for systems to receive data and return predictions. The Nvidia Triton inference server allows the simple deployment of scalable AI models in production.
Play video starting at :9:53 and follow transcript9:53
So what are frameworks? Frameworks are designed to provide higher level building blocks that make it easy for data scientists and domain experts in computer vision, natural language processing, robotics, and other areas to design, train, and validate AI models. They can be an interface library or tool which allows developers to more easily and quickly build models.
Play video starting at :10:18 and follow transcript10:18
Data scientists use frameworks to create models for a variety of use cases such as computer vision, natural language processing, and speech recognition. For example. Mxnet is a modern open source deep learning framework used to train and deploy deep neural networks. It is scalable, allowing for fast model training, and supports a flexible programming model and multiple languages. The MXNet library is portable and can scale to multiple GPUs and multiple machines.
Play video starting at :10:49 and follow transcript10:49
Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression, and clustering algorithms, and is designed to interoperate with the Python numerical and scientific libraries, NumPy vs SciPy. TensorFlow is a popular open source software library for DataFlow programming across a range of tasks. It is a symbolic math library and is commonly used for deep learning applications. NVIDIA Isaac Lab is a lightweight application built on Isaac SIm for robot learning. Isaac Lab optimizes for reinforcement, imitation, and transfer learning and can train all types of robot embodiments. Data scientists can use frameworks to create models for a variety of use cases such as computer vision, natural language processing, and speech recognition.
Play video starting at :11:41 and follow transcript11:41
The diagram shows the software stack for deep learning. The hardware is comprised of a system which can be a workstation or a server with one or more GPUs. The system is provisioned with an operating system and an NVIDIA driver that enables the deep learning framework to leverage the GPU functions for accelerated computing. Containers are becoming the choice for development in organizations. NVIDIA provides many frameworks as Docker containers through NGC, which is a cloud registry for GPU accelerated software. It hosts over 100 containers for GPU accelerated applications, tools and frameworks.
Play video starting at :12:20 and follow transcript12:20
These containers help with faster and more portable development and deployment of AI applications on GPU's across the cloud, data center and edge, and are optimized for accelerated computing on gpu's. Hence, the stack includes running the NVIDIA Docker runtime, specific for NVIDIA GPUs. The containers include all the required libraries to deliver high performance GPU acceleration during the processing required for training.
Play video starting at :12:48 and follow transcript12:48
The CuDA toolkit is an NVIDIA groundbreaking parallel programming model that provides essential optimizations for deep learning, machine learning, and high performance computing leveraging NVIDIA GPUs. There are two ways you can go about building an AI platform. You can either take the do it yourself approach or leverage Nvidia AI enterprise, both of which we'll discuss over the next two sections. Leveraging open source software has become a mainstream method for AI and machine learning development, because it can be collaboratively shared and modified upon distribution. However, building your own AI platform based on open source can be risky without a robust support for production. Aihdenhe open source software is often distributed and maintained by community developers without the dedicated resources for quality assurance and verifications, open source software deployment is often limited to the current GPU architecture and offers only self service support with Nvidia AI enterprise enterprises who leverage the open source practices can build mission critical applications on top of the Nvidia AI platform. Nvidia AI Enterprise provides Nvidia enterprise support and hardware testing and certifications for past, current, and future gpu's. Now that you have an understanding of the two ways you can build an AI platform. Let's explore the benefits of the NVIDIA AI enterprise solution. In order to use a Do It Yourself or build your own approach, or download and use NVIDIA AI enterprise, all software for either of these approaches is provided in the NVIDIAs NGC and the enterprise catalog. Let's take a few minutes to explore that now.
Play video starting at :14:30 and follow transcript14:30
Navigating the world of software stack for AI and accelerated applications is complex. The stack varies by use cases. AI stack is different from HPC simulation apps, and genomics stack is different from the visualization app. The underlying software stack to run a particular application on different platforms, from on prem to cloud, from bare metal to container, and from vm to microservices also varies.
Play video starting at :14:57 and follow transcript14:57
NGC catalog offers containerized software for AI, HPC, data science, and visualization applications built by NVIDIA and by our partners. The containers allow you to encapsulate the application and its complex dependencies in a single package, simplifying and accelerating end-to-end workflows, and can be deployed on-premises in the cloud or at the Edge. NGC also offers pre-trained models across a variety of domains and AI tasks such as computer vision, NLP, and recommender systems. Such pre trained models can be fine tuned with your own data, saving you valuable time when it comes to AI model development. Finally, for consistent deployment, NGC also has helm charts that allow you to deploy your application and NGC collections, which bring together all the necessary building blocks, helping you build applications faster.
Play video starting at :15:52 and follow transcript15:52
The pre-trained models in the NGC catalog are built and continually trained by NVIDIA experts. For many of our models. We provide model resumes. They're analogous to a potential candidate's resume. You can see the data set the model was trained on, training epochs, batch size, and more importantly, its accuracy. This ensures that users can find the right models for their use case. The NGC catalog has rich collections of general purpose, such as ResNet-50 and U-NET. More importantly, the catalog also provides application-specific models, such as people or vehicle detection, pose, and gaze estimation. You'll also find models in conversational AI that include speech recognition, text-to-speech, language translation, and more. Not only do you get these rich assortments of models, but these models can also be easily fine-tuned with your custom data or can be easily integrated into industry SDKs like Riva or DeepStream. Containers are now ubiquitous when it comes to developing and deploying software. A container is a portable unit of software that combines the application and all its dependencies into a single package that is agnostic to the underlying host OS. Using containers in an AI development environment ensures that AI applications run consistently across different computing environments. In scientific research, containers allow researchers to easily reproduce and corroborate without having to rebuild the environment from scratch. NVIDIA NGC containers offer certified images that have been scanned for vulnerabilities and are thoroughly tested. Some of our containers are backed by enterprise support via the NVIDIA AI Enterprise program. The containers are designed to support multi-GPU and multi-node applications for high performance. NGC containers can be run with many container runtimes, including Docker, Cryo, Containered, and Singularity on bare metal, virtual machines, and Kubernetes environments. With a monthly update cadence for deep learning containers such as TensorFlow and PyTorch, the containers are continually improving to offer the best performance possible while targeting the latest versions of software. To provide easy access and support to your AI journey without having to build it yourself, NVIDIA AI Enterprise is the easiest on-ramp. The NIC sections will briefly walk you through what it is, what it does, and how to find it. NVIDIA AI Platform consists of three important layers. Accelerated infrastructure that provides accelerated computing to power the entire AI technology stack. AI platform software, which is the NVIDIA AI Enterprise software suite for production AI. And AI services for enterprises to easily build AI applications leveraging state-of-the-art foundation models. We'll be focusing on NVIDIA AI Enterprise, the software layer of the most advanced AI platform. NVIDIA AI Platform provides reliability and security for production AI, consisting of four important layers. Infrastructure optimization and cloud-native management or orchestration layers are essential to optimize your infrastructure to be AI ready. Cloud-native management and orchestration tools facilitate deployment of the solution in cloud-native and hybrid environments. AI and data science development and deployment tools includes the best-in-class AI software that's needed for development and deployment. AI workflows, frameworks, and pre-trained models are designed for enterprise to quickly get started with developing specific AI use cases and addressing business outcomes. For example, customers might leverage included AI workflows to develop intelligent virtual assistants for contact centers or digital fingerprinting to detect cybersecurity threats. The entire software stack can be flexibly deployed across accelerated cloud, data center, edge, and embedded infrastructure, wherever you choose to run your AI workloads. Applications can run anywhere that NVIDIA infrastructure is available. With one license, NVIDIA AI Enterprise covers your AI center of excellence or COE needs, partnered with the most experienced group of enterprise AI experts in the market. With included enterprise support, NVIDIA AI Platform offers cloud-native, hybrid optimized, deploy anywhere on-prem and in the cloud. Reduced development complexity, secure and scalable, certifications with broad partner ecosystem, improved AI model accuracy, standard support 9x5 premium 24x7. Now that you have a general understanding of NVIDIA AI Enterprise and its benefits, let's turn our attention to GPU virtualization. NVIDIA offers a diverse range of SDKs, models, and frameworks. This slide provides a concise overview of their functions. For a deeper understanding of any specific model or framework, a quick Google search is recommended. To round up this discussion on the AI ecosystem, we will briefly cover NVIDIA's AI workflows. One question that frequently arises is whether there is a difference between an AI workload and a workflow. We believe there is a difference, and NVIDIA provides solutions to address both scenarios. There are customers who are running workloads already, and these can be accelerated by NVIDIA frameworks and libraries that leverage NVIDIA GPUs. Also, there are organizations who would like to deploy specific workflows, but aren't quite sure how to build them or how to get started. For these customers, we've created AI workflows, which are assembled, tested, documented, and customizable to provide customers a head start in solving specific challenges. Now that you understand the differences between workloads and workflows, let's explore another potential point of confusion. Let's explore NVIDIA's AI workflows. Available through NGC and the Enterprise Catalog, these are prepackaged solutions designed to assist AI practitioners with specific use cases. Each workflow guides you through the necessary tools and steps to create and run a variety of workflows. These workflows have been fully tested and are vetted by NVIDIA. In the future, NVIDIA plans to introduce more AI workflows to cover a broader range of use cases. The majority of enterprises are now moving to adopt AI, but the vast majority are struggling with the complexity of getting to production. Our AI workflows are designed to give these customers a jumpstart on their AI journey, with prepackaged reference examples illustrating how NVIDIA AI frameworks can be used to build AI solutions. Included in our workflows are our AI frameworks, pre-trained models, training and inference pipelines, Jupyter Notebooks, and Helm charts. All of these components are curated to help customers accelerate the path to delivering AI outcomes. The advantages for customers are two-fold. They can rapidly develop and deploy their solutions and produce solutions that provide the highest accuracy and performance. And if they encounter challenges, NVIDIA's enterprise support team is just a call away. Let's take a moment to summarize the unit and talk about the next step in your learning journey. Now that you've completed this unit, you should be able to describe VGPU, which serves as a foundation for the AI ecosystem. Describe the NVIDIA deep learning software stack and NVIDIA CUDA-X ecosystem. Define the steps in an AI pipeline workflow and identify some of the available tools to facilitate each step. Define what frameworks are and identify open source third-party and NVIDIA frameworks. Describe the benefits of NGC and the enterprise catalog to provide building blocks in a DIY AI solution. Describe the benefits and use cases of NVIDIA AI Enterprise. Describe provided AI workflows. Don't stop here, continue the introduction to AI in the data center learning journey. See you in the following unit.