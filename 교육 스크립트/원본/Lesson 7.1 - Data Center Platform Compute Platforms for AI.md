# Lesson 7.1 - Data Center Platform | Compute Platforms for AI

0:07
Welcome to Unit 7: Compute Platforms for AI. Topics covered in this unit include data center platform, GPUs and CPUs for AI data centers, multi-GPU systems, introducing DPUs, and NVIDIA-certified systems. By the end of this unit, you will be able to indicate the key components and features of the NVIDIA data center platform; identify the GPU and CPU requirements for AI data centers, the different products available, and their intended use cases; understand the purpose and capabilities of multi-GPU systems; describe the multi-node GPU interconnect technology; determine the role of DPUs and DOCA in an AI data center; and evaluate the benefits of using NVIDIA-certified systems. Let's get started. First, we'll present the NVIDIA data center platform and review some consideration and requirements for building compute platforms for AI. Modern data centers are key to solving some of the world's most important scientific, industrial, and big data challenges using high-performance computing and AI. Accelerated computing is often referred to as a full stack challenge because it involves optimizing and integrating various components across multiple layers of the technology stack to achieve optimal performance for specialized workloads. Furthermore, accelerated computing represents a data center scale issue, as the modern data center essentially serves as the computer. Applications span the entire data center, making it crucial to optimize all the diverse components within it. At the foundation are the hardware technologies, GPUs, CPUs, and DPUs that form the basis for building servers. Sitting atop these servers is this software stack, encompassing CUDA and DOCA, the programming models for GPUs and DPUs respectively, along with numerous software libraries that transparently provide acceleration to developers across different hardware products, such as CUDA-X for GPU acceleration. In addition, we offer application frameworks tailored for common domains. Some examples include Riva for conversational AI, DRIVE for autonomous vehicles, Merlin for recommendation systems, and many others. Customers leverage our comprehensive stack to develop and run their applications effectively.
Play video starting at :2:40 and follow transcript2:40
As mentioned earlier, the data center is now the new unit of computing, and to make this model work, it requires three pillars: the CPU, the GPU, and the DPU. The GPU is used for accelerated computing, performing parallel processing at the enormous scale required for graphics and AI. The CPU continues to perform general application processing, especially basic single thread applications, which it is good at. The DPU comes in to handle data-intensive functions like communications processing, compression, and encryption to keep the data center running efficiently. The combination of the GPU, the DPU, and the CPU is now the new unit of computing. We'll talk about each of those in more detail in the following slides.
Play video starting at :3:30 and follow transcript3:30
An accelerated system is the next phase in the evolution of computers. Just like how all smartphones today have processors for graphics and AI, so, too, will every server and workstation have compute accelerators to power today's modern applications, including AI, visualization, and autonomous machines. Many of these systems will also have data processing units, which accelerate the network, storage, and security services that are central to cloud native and cloud computing frameworks. Leveraging cloud service providers or CSPs grants customers access to computing infrastructure and resources without the need for management and maintenance. We will delve into cloud-based solutions in greater detail in a later unit. Additionally, OEM systems are readily accessible. NVIDIA collaborates with numerous reputable, established, and certified vendors, offering flexibility in adopting solutions built from readily available components. Lastly, the NVIDIA DGX systems are purpose-built with optimized components, including networking, storage, and compute. Customers who opt for DGX solutions gain access to NVIDIA's expertise, which can assist them in deploying and maintaining their solutions effectively.
Play video starting at :4:47 and follow transcript4:47
As organizations seek to build an AI application, they follow a workflow that begins with ideation and is ultimately realized as a trained model running in a production setting. The process to go from an initial concept to a production application involves several phases enacted by a team that includes data scientists, data engineers, business analysts, DevOps, and potential application developers working in concert. The workflow shown here is an idealized example to showcase the key phases of this development process. With cloud-based GPU solutions, enterprises can access high-density computing resources and powerful virtual workstations at any time from anywhere with no need to build a physical data center. From virtual desktops, applications, and workstations to optimized containers in the cloud, data scientists, researchers, and developers can power GPU-accelerated AI and data analytics at their desks. GPU-accelerated data centers deliver breakthrough performance for compute and graphics workloads at any scale with fewer servers, resulting in faster insights and dramatically lower costs. Sensitive data can be stored, processed, and analyzed while operational security is maintained. AI at the edge needs a scalable, accelerated platform that can drive decisions in real-time and allow every industry to deliver automated intelligence to the point of action, stores, manufacturing, hospitals, and smart cities. Jetson is the NVIDIA platform designed for edge deployments.