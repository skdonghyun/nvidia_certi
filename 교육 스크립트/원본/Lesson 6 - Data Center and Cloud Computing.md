# Lesson 6 - Data Center and Cloud Computing
0:06
Welcome to Unit 6, where we delve into data center and cloud computing, the environments that drive AI workloads. This unit begins with a short recap of prior units, introducing key AI concepts and features. We then explore data centers and cloud computing as environments for running AI workloads. The unit progresses by familiarizing us with the components of AI infrastructure. We close by shedding light on the aspects of operating an AI data center. By the end of this unit, you'll be able to summarize AI principles and features discussed in prior units, outline the hosting environments for AI workloads, such as data centers and the cloud, enumerate the components constituting AI data centers, and indicate the requisites and methods for managing and monitoring AI data centers. Our exploration has thus far underscored the immense value AI brings to diverse industries. Familiar technologies in our daily lives are increasingly powered by AI with continual evolution into machine learning, deep learning, and generative AI, each phase, unlocking new capabilities. Generative AI emerges as a powerful tool, facilitating the rapid generation of diverse content for creatives, engineers, researchers, scientists, and more. Its applications span industries, producing novel content like stories, emails, music, images, and videos. The advent of accelerated computing, notably powered by GPUs, has become pivotal as CPU scaling reaches its limits. GPUs play a crucial role in providing the necessary processing power for complex AI workloads. Additionally, the importance of a suitable software stack cannot be overstated, acting as the backbone that orchestrates the seamless interaction between hardware and AI algorithms, ensuring optimal performance and efficiency in this rapidly evolving technological landscape. So where does this AI magic happen?
Play video starting at :2:13 and follow transcript2:13
We begin by discussing the environments of where AI workloads run, data centers or the cloud, designed and built for computing. A data center is commonly described as a physical or cloud facility designed to host essential business applications and information. These centers encompass various components that can be broadly categorized into three groups: storage, compute, and networking. Given that AI workloads are both data and compute-intensive, traditional data centers may fall short. The massive datasets used by AI require high performance and high-speed storage, while extensive computations demand execution on multiple accelerated systems. To achieve this, multiple compute, storage, and management nodes are networked to form a cluster. The interconnected network must provide high performance and low latency to avoid becoming a bottleneck. At the same time, these specialized data centers are equipped with power and cooling infrastructure to optimal hardware functionality. In upcoming units, we'll delve into the essential infrastructure components for AI-supportive data centers and explore the fundamentals of managing and monitoring these dynamic environments. Let's embark on this journey together. In the next phase, we'll explore how IT leaders build and scale their data center infrastructure to readily adopt AI. AI applications demand significant computing power, driven by both training and inference workloads. Accelerated systems utilizing high-powered processors, memory, and GPUs efficiently process large amounts of data, distributed across interconnected nodes. GPU-accelerated servers are offered by your OEM of choice.
Play video starting at :4: and follow transcript4:00
AI workloads involve large computations, which are distributed over hundreds and thousands of nodes. Distributed computing involves the utilization of multiple interconnected nodes working together to perform a task or execute a process. In this model, the workload is distributed across various machines, connected by a high-speed, low-latency network. AI workloads have introduced new challenges and requirements for data center network architectures. The network defines the data center and serves as the backbone of the AI infrastructure. It is essential to consider the network's capabilities and end-to-end implementation when deploying data centers for AI workloads.
Play video starting at :4:43 and follow transcript4:43
Accelerate systems provide massive computational power for AI training and inferencing. Completing those jobs in a timely manner requires high sustained rates of data delivery from the storage. High speed storage access is crucial for AI workloads, enabling rapid data access and transfer rates for improved performance and reduced latency. Sequential read is the most common pattern of storage access when training a neural network. Data centers must provide sufficient storage capacity and address considerations such as capacity, performance, network hardware, and data transfer protocols. AI applications demand more power for computations, increasing power usage and generating heat. Inefficient cooling can result in reduced equipment life, poor computing performance, and greater demand on cooling systems. Sustainable computing maximizes energy efficiency, which is crucial to reducing the environmental impact of technology growth. Adopting sustainable practices helps data centers lower their carbon footprint and energy use.
Play video starting at :5:49 and follow transcript5:49
Some factors to maximize energy efficiency include accelerated computing and efficient cooling. Accelerated computing is the most cost-effective way to achieve energy efficiency in a data center. By utilizing specialized hardware to carry out certain common complex computations faster and more efficiently, data centers can perform more computations with less energy. Efficient cooling technologies like direct liquid cooling, efficiently dissipates heat, offering energy-saving advantages such as improved heat transfer, reduced airflow needs, targeted cooling, and waste heat reuse.
Play video starting at :6:27 and follow transcript6:27
Let's begin with an overview of reference architectures. Dense computing environments include many components. There are multiple servers for compute networking fabrics that connect the systems, storage for data, and management servers.
Play video starting at :6:45 and follow transcript6:45
Designing systems to get maximum performance can be very difficult. Reference architectures are documents showing a recommended design for the implementation of a system. It uses best-of-breed designs to provide high-performance solutions.
Play video starting at :7:5 and follow transcript7:05
Dense computing environments include many components. There are multiple servers for compute networking fabrics that connect the systems, storage for data, and management servers. Designing systems to get maximum performance can be very difficult. Reference architectures are documents showing a recommended design for the implementation of a system. It uses best-of-breed designs to provide high-performance solutions. Reference architectures can be used as a foundation for building designs, using systems, and components. As AI technology continues to advance and integrate into enterprise operations, the challenge of building and maintaining a robust on-prem AI infrastructure becomes critical. Cloud-based solutions, especially those leveraging GPUs, offer a flexible and accessible alternative to physical data centers.
Play video starting at :7:58 and follow transcript7:58
The AI data center infrastructure section provides six comprehensive aspects to guide the design of data centers for AI workloads. After establishing a data center, effective management and monitoring become imperative. Let's explore some of the related aspects.
Play video starting at :8:18 and follow transcript8:18
Managing IT infrastructure for AI poses unique challenges for IT admins, data scientists, and line-of-business owners. The complexity of modern data science workloads, incorporating GPU acceleration and high-speed networking, requires specialized attention. Infrastructure provisioning. IT admins navigate diverse, often container based data science workloads distinct from traditional enterprise operations. Managing complex computing infrastructure involving GPU acceleration and high-speed networking is a critical responsibility. Workload management. Data scientists are tasked with more than using their laptops. They require access to centralized compute resources but often lack the IT knowledge to independently utilize these systems. Moreover, they need scalable access to resources as their needs expand from experimentation to larger-scale testing. Resource monitoring. Line-of-business owners must ensure optimal use of compute resources, relying on relevant and accurate data about resource usage to make informed business decisions for their stakeholders. Additional aspects for operating an AI data center include container orchestration and job scheduling. Container orchestration and scheduling play pivotal roles in the efficient management of AI data centers. Container orchestration involves automating container-related operations, including provisioning, deployment, management, scaling, and load balancing. Orchestration tools handle these tasks based on the managed environment's specific needs. For advanced scheduling, an additional scheduling tool can be employed in conjunction with an orchestration tool. Scheduling is the process of assigning workloads to available compute resources. Schedulers provide a framework for launching and monitoring jobs, managing jobs in the queue, and ensuring that jobs receive the necessary resources to run.
Play video starting at :10:22 and follow transcript10:22
The AI data center operation section consists of two units, providing details and considerations on how to effectively operate your AI data center. Let's summarize what we've learned. Now that you've completed this unit, you should be able to summarize AI features discussed in prior units, outline the hosting environments for AI workloads, such as data centers and the cloud, enumerate the components constituting AI data centers, and indicate the requisites and methods for managing and monitoring AI data centers. Now proceed to Section 2, delving into AI infrastructure, where you'll begin your exploration of its components, starting with Unit 7 that addresses compute platforms designed for AI. See you in the next unit.