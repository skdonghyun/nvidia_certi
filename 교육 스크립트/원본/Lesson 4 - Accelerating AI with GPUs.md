# Lesson 4 - Accelerating AI with GPUs

0:08
Welcome to the Accelerating AI Using Nvidia GPUs unit. This unit introduces you to Nvidia GPUs, the engines for accelerated compute. Let's jump over to the agenda to see what we have in store in this unit. In this unit, we cover a historical perspective on GPUs, looking at why GPUs were developed in the first place. And how they've evolved to become a key component for accelerating computing in the modern data center. A deep dive into general GPU architecture, a head-to-head comparison between CPUs and GPUs. An overview of GPU server systems within the data center. Lastly, an introduction into the last three generations of Nvidia GPU architecture. Lets examine the learning objectives for this unit.
Play video starting at ::59 and follow transcript0:59
After completing this unit, you should be able recall significant milestones in GPU history and describe key developments in the evolution of GPU technology. Explain the core components of GPU architecture and their functions, demonstrating a clear understanding of how GPUs work. Analyze and compare the architectural differences between CPUs and GPUs, highlighting their strengths and limitations in various computing scenarios. Apply knowledge of GPU server systems to plan, configure, and deploy GPUs effectively, taking hardware configurations and deployment strategies into consideration. Evaluate Nvidia's AI GPU families, assess their features, and determine which GPU family best suits specific AI and deep learning use cases based on their capabilities and characteristics. We have a lot to cover, so let's get started. Delving into the rich and ever evolving history of GPUs, we uncover the milestones and technological transformations that have shaped the world of graphics processing.
Play video starting at :2:7 and follow transcript2:07
A graphics processing unit, or GPU, is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. Computer graphics have undergone significant evolution since the 1970s. Their importance cannot be overstated as humans primarily rely on vision to process the information presented by computers. Images on screens are comprised of picture elements, or pixels. A pixel is the smallest unit of a digital image that can be displayed and represented on a digital display device. Pixel characteristics include position, color, and brightness. Every pixel in an image must be processed at such a rate that the human eye does not perceive any delays or inconsistencies as it absorbs the image being presented. As display and computer technology have, advanced screens now have more pixels, leading to more realistic image representation. This is referred to as screen resolution, which represents pixel density. The processing behind the pixels is done by the GPU. As screen resolutions have increased, the processing power necessary to represent each pixel has also increased. GPUs have evolved over time to become a fundamental building block in computer architecture. The 1980s saw the development of individual graphics display processors, and the 1990s sees the development of separate boards or cards that can be modularly replaced in computer systems. Now that you have an understanding of the history and evolution of the GPU, let's turn our attention to this groundbreaking architecture.
Play video starting at :3:53 and follow transcript3:53
GPU architecture forms the core foundation of modern graphics processing. In this exploration, we'll delve into the intricate design and functionality that powers these essential components of accelerated computing.
Play video starting at :4:7 and follow transcript4:07
This is an image of a typical GPU. At the heart of the chip are thousands of GPU cores. A core is the component of the GPU that processes data, onboard cache memory, which acts as a typical cache, storing a copy of everything for quick, reliable data access. Parallel processing is possible with the use of multiple cores. Closest to the cores is the high speed GPU memory. This memory is designed specifically for GPU use and can be shared with other GPU's. Now that you have an understanding of the general GPU architecture, let's turn our attention to the factors that have led to the rise of Nvidia GPU computing. AI models continue to grow in complexity, and at an astonishing rate. In just the past three years, the size of state of the art AI models has grown by three orders of magnitude, and this exponential pace will continue. This growth in data and AI model sizes requires more compute, which is possible through GPU's but not through CPU's, also required a mature set of frameworks and tools to maximize performance and accelerate deployment. Nvidia is at the center of the AI stack, from architecture and platforms to CUDA, from frameworks to the Triton server and NGC. GPU computing has given the industry a path forward to keep up with the expected performance evolution.
Play video starting at :5:36 and follow transcript5:36
This success is achieved through a highly specialized parallel processor design. Which permeates the approach to the system design, system software, algorithms, and optimized applications. Continuous optimizations between hardware and software produce ongoing performance gains. Lets explore some key data center trends that also contribute to the rise in GPU computing. The data center landscape is accelerating. Consider the pace at which new services are being adopted and to see how AI is accelerating the adoption of new services and capabilities. For instance, lets examine the time it took these apps to amass 100 million users. WhatsApp attained 100 million users in 49 months, ChatGPT did so in just two months.
Play video starting at :6:29 and follow transcript6:29
AI is accelerating how fast new services come along and connect with the community, and this is stimulating demand for advanced computing power. At the same time there is sensitivity to climate change and the need for greener computing. Theres also a challenge to get access to more compute in data centers around the world. Data center energy usage is exceeding 200 terawatts per year. The data center represents about 2% of the global energy usage and this percentage is projected to increase to 5% by 2030. Data centers are power limited and take years to plan and build. To meet the demand to deliver these new services, data center operators need to optimize the infrastructure they have with the power constrained data centers they have. Accelerated computing is one way to achieve that goal. Lets explore why accelerated computing is the path forward.
Play video starting at :7:25 and follow transcript7:25
Nvidia CEO Jensen Huang famously stated Moores Law is dead, making the bold claim several times dating back to 2017. The end of Moores law refers to the slowing down of exponential transistor density growth on microchips. Leading to challenges in achieving regular and significant performance improvements in traditional semiconductor technology. It's essentially a physics problem as transistors would eventually reach the limits of miniaturization. To see Nvidia CEO Jason Huang discuss this principle, click on the video from his GTC 2023 keynote. Address.
Play video starting at :8:4 and follow transcript8:04
CPUs are simply unable to keep up with the complex workload demands associated with accelerated computing. This limitation will only get worse as the size and complexity of models increase. Accelerated computing requires a comprehensive and integrated full stack approach that encompasses hardware, software, and frameworks to harness the full potential of accelerators like GPU's for complex workloads. For example, an NVIDIA AI platform with H100 GPUs set new time to train records at scale across every workload. This includes the new LLM workload where training times were reduced from days to hours and in the case of the largest LLMs, from a month down to a week. It's important to highlight that NVIDIA consistently pushes the boundaries of GPU technology as demonstrated by our latest innovation, the Grace Hopper GH200, which well delve into in Unit 7 which focuses on compute platforms for AI. Let's perform a deep dive comparison between the CPU and GPU to better understand the strengths and weaknesses of each.
Play video starting at :9:11 and follow transcript9:11
In the world of computing, understanding the distinctions between CPUs and GPUs is pivotal. In this topic, we embark on a journey to compare and contrast these fundamental processing units.
Play video starting at :9:25 and follow transcript9:25
Central processing units, or CPU's, are a computer component designed to process complex instruction sets that execute, code and manipulate data. Originally, instructions were processed one at a time in the processing unit of the chip called the core. The core reads and executes the program instructions. As CPU architecture evolved, multicore processors were developed. This allowed several instructions to be processed simultaneously, leading to an increase in processing performance.
Play video starting at :9:56 and follow transcript9:56
Gpu's are designed to execute simple instruction sets. Consequently, the number of cores that can be built in a comparatively similar silicon area is much larger than with the CPU. With relatively many more cores than a CPU, a GPU allows processing many simple instructions simultaneously. Both CPUs and GPUs are system components that work in tandem to process code and data.
Play video starting at :10:26 and follow transcript10:26
Let's look at some CPU characteristics.
Play video starting at :10:29 and follow transcript10:29
Over the last few years, CPUs have moved to a multi core architecture with the latest CPUs containing up to 128 cores with fast clock speeds. Cpu's also have large main memory however, the bandwidth of that memory is relatively low, which affects how quickly we can move data about.
Play video starting at :10:51 and follow transcript10:51
CPU's are designed to run multiple different applications concurrently. Each of these applications is assigned to a single thread or a small number of threads and is scheduled in a time sliced manner to run on the CPU. This requires low latency to minimize the delay between issuing a request for data and executing the instructions on the data. This implies large caches to hold the data required by the threads and complex control logic to ensure that the data is ready to go when the thread is running.
Play video starting at :11:22 and follow transcript11:22
One consequence of this is that a large amount of the silicon on a CPU is dedicated to data movement, meaning that CPU's have relatively low performance per watt, as a significant proportion of the energy is used for data movement rather than actual calculations. In addition, there are cache misses when trying to access data which isn't in the cache yet, which can be very detrimental to performance.
Play video starting at :11:47 and follow transcript11:47
Let's look at the GPU in comparison. A GPU is optimized for executing highly parallel tasks stemming from its roots in generating actual computer graphics, where the same operation is applied to millions of pixels multiple times a second in order to render scenes. Modern GPUs have a huge number of compute cores, over 10,000 for some of the latest cards. However, these compute cores don't have the complex logic for pre-fetching data, for example. So instead the GPU deals with the issue of latency by hiding it with computation. Essentially, we assign the GPU more tasks than its physical cores can handle, contrasting the approach taken with the CPU. The GPU scheduler launches a thread that tries to execute an operation, for example, an addition. If the data is not ready for the thread to use, it issues a fetch command to the memory and it stalls while waiting for the data to arrive. The scheduler then moves on to another thread to execute its instructions, and so on and so forth, creating a pipeline of issued instructions across a series of threads. Eventually, the original thread's data is ready and can continue execution. This switching between the threads hides the latency of the memory fetching and the issuing of instructions. Thus, the more work given to the GPU to perform, the easier it is for the scheduler to hide this latency. So what's needed is many overlapping concurrent threads. This contrasts with how we program for the CPU, where we typically allocate one or two threads per compute core. To support this, the GPU's have a very large register file to hold the state of the threads, and switching between them happens at no time penalty, even as often as every clock cycle. As such, most of the silicon on a GPU is given over to computation rather than data movement, giving them very efficient performance per watt. In addition, a GPU has very high bandwidth main memory compared to a CPU. As many applications are bandwidth bound rather than compute bound, this can deliver significant performance improvements. The additional bandwidth, though, comes at a cost, with the GPU memory being significantly faster but relatively smaller. For example, the H 100 GPU has an 80gb memory version large for a GPU, but small compared to the CPU. However, it runs at a massive 2 terabytes per second bandwidth, pretty much an order of magnitude faster than the CPU memory. Let's dive deeper and explore how GPU acceleration works.
Play video starting at :14:33 and follow transcript14:33
A typical application has parts which need to be executed sequentially and parts which can be executed in parallel. Although the parallel parts may only be a small part of the overall application, they're usually the most compute, intensive and time consuming. Therefore, by executing them in parallel on a GPU, we can see huge increases in performance, often orders of magnitude faster than on a CPU alone. There are many algorithms used across a huge range of domains which can be parallelized and thus see significant performance improvements. From this approach, let's contrast how data is processed by a CPU and GPU.
Play video starting at :15:13 and follow transcript15:13
Let's consider how the flow of data occurs for those tasks that are offloaded between the CPU and GPU. Note that the CPU and GPU are two distinct entities. They each have their own memory, so we need to bear this in mind when programming for the GPU. First, we need to move the data we wish to work on from the CPU memory to the GPU memory. This is usually via the data path provided by the peripheral component interconnect or PCI bus. The code to be executed by the GPU on the data is then copied from the CPU to the GPU. Once loaded, it is launched and the operations on the data take place within the GPU. The data takes advantage of the various layers of cache for faster performance.
Play video starting at :16:1 and follow transcript16:01
>> After the GPU finishes processing, the resultant data is copied back to the CPU memory for any additional processing if required. Although this basic processing flow illustrates data passing through a PCIe bus, it's important to mention that alternative GPU interconnection methods like NVLink exist. We will delve into these in a subsequent unit. Now that you have an understanding of the way data is processed from both a GPU and CPU perspective, lets turn our attention to GPU server systems.
Play video starting at :16:34 and follow transcript16:34
GPU server systems represent the backbone of high performance computing and deep learning infrastructure. In this section, we'll navigate the fundamentals of GPU servers and their ever growing ecosystem.
Play video starting at :16:47 and follow transcript16:47
A GPU server is a specialized computer system equipped with graphics processing units or GPUs, designed to accelerate complex computations in the following ways.
Play video starting at :16:58 and follow transcript16:58
GPU's Excel in parallel processing tasks, making them ideal for tasks such as deep learning, scientific simulations, and data analysis. GPUs contain thousands of cores that can perform multiple calculations simultaneously. This parallel processing capability enables faster execution of tasks compared to traditional central processing units or cpu's, especially for applications that involve massive data sets and complex algorithms. GPU servers are optimized for specific workloads such as artificial intelligence, machine learning, and graphics rendering. They can dramatically reduce processing times, enabling researchers, engineers and developers to solve complex problems and innovate more efficiently. NVIDIA offers specialized GPUs designed for data center environments. While typically available in a PCIe form factor, GPU's can also come in an SXM or MXM form factor. Flagship examples of types of GPU systems DGX H100, is a fully integrated hardware and software solution on which to build your AI center of excellence. NVIDIA HGX H100 combines H 100 tensor core GPUs with high speed interconnects to form the world's most powerful servers.
Play video starting at :18:19 and follow transcript18:19
The NVIDIA H100 PCIe debuts the world's highest PCIe card memory bandwidth greater than 2000 gigabytes per, GDPS. This speeds time to solution for the largest models and most massive datasets.
Play video starting at :18:35 and follow transcript18:35
Nvidia MGX a modular reference design that can be used for a wide variety of use cases, from remote visualization to supercomputing at the edge, MGX provides a new standard for modular server design. By improving ROI and reducing time to market, Nvidia cannot do it alone. Therefore, we tap into a robust partner and cloud service provider ecosystem to power their accelerated compute solutions.
Play video starting at :19:3 and follow transcript19:03
In today's enterprise market, modern data centers are responsible for almost any computing challenge. With AI, high performance computing, and data science growing exponentially, there is no doubt that an enterprise data center will need to have GPU systems to support this demand. All major data center server vendors offer GPU based AI systems Nvidia, Cisco, Dell, HPE, IBM, Lenovo and others. These servers support two to 16 GPUs each, and can be interconnected to create multi server parallel processing systems. Also, NVIDIA GPUs have been adopted by every major cloud provider, extending accelerated computing into the cloud. Let's explore how GPU's are consumed within the data center.
Play video starting at :19:52 and follow transcript19:52
There is an ever expanding range of workloads that lend themselves to GPU acceleration. Compute intensive tasks like AI training and inference, data analytics and high performance computing or HPC. General purpose tasks like visualization, rendering, virtual workstation and deep learning. High density virtualization through solutions like virtual desktop and workstation. Enterprise edge solutions in controlled environments. Industrial edge solutions within industrial or rugged environments. Desktop workstations that support design, content creation, and data science workloads. Mobile workstations that facilitate design, content creation, data science, and software development workloads. Now that you have a solid grasp of the primary applications of GPU's in the data center, let's review the key takeaways from this unit and look ahead to the next lesson in the course.
Play video starting at :20:52 and follow transcript20:52
Now that you've completed this unit, you should be able to, recall significant milestones in GPU history and describe key developments in the evolution of GPU technology. Explain the core components of GPU architecture and their functions. Demonstrating a clear understanding of how GPUs work. Analyze and compare the architectural differences between CPUs and GPUs. Highlighting their strengths and limitations in various computing scenarios. Apply knowledge of GPU server systems to plan, configure, and deploy GPUs effectively, considering hardware configurations and deployment strategies. Evaluate NVIDIAs AI GPU families assess their features and determine which GPU family best suits specific AI and deep learning use cases based on their capabilities and characteristics.
Play video starting at :21:44 and follow transcript21:44
Great progress. Don't stop here, continue the journey with Unit 5, AI software ecosystem, which details a dynamic and rapidly evolving landscape where cutting edge algorithms, frameworks, and tools converge to enable groundbreaking artificial intelligence applications. See you in the following unit.