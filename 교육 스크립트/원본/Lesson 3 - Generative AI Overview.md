# Lesson 3 - Generative AI Overview

0:08 Welcome back to the AI infrastructure and Operations Fundamental course. This is the third unit in the course, which covers an overview of Generative AI.
Play video starting at ::18 and follow transcript0:18
Here's the outline for this unit. Unit 3 is aimed at understanding what generative AI is, how the technology works, an overview of large language models, or LLMs, and the steps required to deploy generative AI solutions in the enterprise.
Play video starting at ::36 and follow transcript0:36
By the end of this unit, youll be able to explain what generative AI is and how the technology works, discuss at a high level the main concepts of large language models, and describe the steps required for enterprises to unlock new opportunities for their business.
Play video starting at ::54 and follow transcript0:54
In the previous unit, we learned about the AI process that allows us to reach generative AI, but let's get a better understanding of exactly what it is.
Play video starting at :1:4 and follow transcript1:04
Generative AI refers to a subset of artificial intelligence that focuses on creating data or content, such as images, text, and multimedia based on patterns and examples from a given data set. What sets generative AI apart is its versatility, enabling it to perform a wide array of tasks beyond text and chat based applications. These systems can generate diverse forms of content, including realistic images, videos, music, and even entire virtual environments, by learning from and synthesizing patterns in the input data. Its adaptability to diverse data types and applications underscores its potential to transform multiple industries and augment human capabilities across a wide spectrum of tasks.
Play video starting at :1:50 and follow transcript1:50
Generative AI is making inroads into every industry, transforming traditional practices, and bringing forth unprecedented operational efficiency and innovation.
Play video starting at :2: and follow transcript2:00
In finance, it enhances fraud detection, personalized banking, and provides valuable investment insights. Within healthcare, it powers molecule simulation, drug discovery, and clinical trial data analysis. Retail benefits from personalized shopping, automated catalog descriptions, and automatic price optimization. In manufacturing, it transforms factory simulation, product design, and predictive maintenance.
Play video starting at :2:28 and follow transcript2:28
These are just some examples of the innovations brought by generative AI.
Play video starting at :3:22 and follow transcript3:22
DALL-E creates realistic images from text descriptions. It can be used for image synthesis tasks such as image captioning, image editing, or image manipulation.
Play video starting at :3:33 and follow transcript3:33
Edify is a diffusion model for synthesizing images given text, which generate photorealistic images corresponding to any input text prompt.
Play video starting at :3:43 and follow transcript3:43
Llama 2 can be used for generating diverse and high quality natural language text, making it valuable for various tasks such as content creation, language understanding, and conversational AI applications.
Play video starting at :3:56 and follow transcript3:56
NVIDIA-GPT is a family of production-ready large language models, or LLMs, that can be tuned to build enterprise generative AI applications that can perform a range of tasks from creating product descriptions, answering customer queries, and writing code.
Play video starting at :4:13 and follow transcript4:13
GPT four gives applications the ability to create human like text and content, images, music, and more, and answer questions in a conversational manner.
Play video starting at :4:24 and follow transcript4:24
We just saw examples of foundation models. Let's now discuss how they are trained. The large language models, or LLMs, powering the advances in generative AI are a significant turning point. They've not only cracked the code on language complexity, enabling machines to learn context, infer intent, and be independently creative, but they can also be fine-tuned for a wide range of different tasks.
Play video starting at :4:50 and follow transcript4:50
A foundation model is trained on a large amount of unlabeled data, that is, data that does not have any predefined categories, labels, or annotations, such as raw text, images, audio, or video. Unlabeled data is abundant and diverse and can be obtained from various sources such as the Internet, social media platforms, or proprietary datasets. A foundation model trained on text data can be used to solve problems related to natural language processing, such as question answering, information extraction, etc. The possibilities for what a foundation model can generate are endless and depend on the creativity and ingenuity of the users who apply them to different problems and domains.
Play video starting at :5:34 and follow transcript5:34
Large language models utilize a specialized neural network known as the transformer to grasp patterns and relationships within textual data. They undergo pre-training on extensive text datasets and can be fine-tuned for specific tasks.
Play video starting at :5:50 and follow transcript5:50
The goal of the language model is given the preceding words in a context to predict the next word. While this example pertains to the English language, the prediction could apply to a computer programming language or another language. The model generates text one word at a time based on an input prompt provided by the user. In this case, the input prompt write a review about an italian restaurant I visited and enjoyed. The input prompt is broken down into smaller tokens that are then fed back into the model. The model then predicts the next word in the sequence based on the tokens it has received. This process continues until the user stops providing input or the model reaches a predetermined stopping point.
Play video starting at :6:32 and follow transcript6:32
LLMs are constructed based on tokens, which represent the smallest units of meaning in a language. Tokens encompass words, characters, subwords, or other symbols representing linguistic elements.
Play video starting at :6:46 and follow transcript6:46
The transformer model architecture empowers the LLM to comprehend and recognize relationships and connections between tokens and concepts using a self-attention mechanism. This mechanism assigns a score, commonly referred to as a weight, to a given item or token, to determine the relationship.
Play video starting at :7:7 and follow transcript7:07
Generative AI models often involve complex mathematical operations and require intensive computations. GPU's are designed to be highly effective for parallel processing. This parallelism enables faster training and inference times for generative AI models compared to using traditional cpu's GPU's excel in parallel processing matrix operations, memory capacity and memory bandwidth, making them an ideal choice for powering up generative AI. They significantly accelerate the training and inference processes, enable working with large scale models, and support real time applications. For example, ChatGPT was trained on 10,000 NVIDIA GPUs for weeks.
Play video starting at :7:52 and follow transcript7:52
While generative AI offers significant benefits, including increased efficiency and cost savings, its adoption does not come without challenges. To successfully implement such solutions, youll need to address various technical, ethical and regulatory issues. It's our responsibility to build using guardrails or rules to mitigate inappropriate outcomes. Data privacy and security. Generative AI use cases in the healthcare and financial sectors should be monitored very closely to forestall any money-related or sensitive data leakages. IP rights and copyright. Generative AI platforms should mitigate copyright infringement of the creator's work. Bias, errors and limitations generative AI is just as prone to biases as humans are because in many ways it is trained on our own biases. Ethical implications determining responsibility for the outputs of generative AI can be challenging. If AI systems generate harmful content, it may be unclear who bears responsibility, the developers, the users, or the technology itself.
Play video starting at :8:58 and follow transcript8:58
Malevolent activities. There is no state-of-the art know-how that wrongdoers can't put to their evil uses, and generative AI is not an exception, where fraudulent scams of various kinds can be created.
Play video starting at :9:12 and follow transcript9:12
Having gained a deeper understanding of generative AI, let's now explore its practical applications in the enterprise.
Play video starting at :9:20 and follow transcript9:20
Generative AI is a powerful branch of artificial intelligence that holds immense potential for addressing various challenges faced by enterprises. While many customers have previously explored AI using traditional classification, named entity recognition, or NER, and natural language processing, or NLP, tasks, there's a gradual migration towards large language models for these familiar tasks. As organizations become more acquainted with LLMs, they're increasingly recognizing the value of generative AI and expanding its application across various workloads. In this slide, we'll explore key scenarios where generative AI can be leveraged to effectively solve enterprise challenges. Generative AI produces new content based on patterns and trends learned from training data. Traditional AI, on the other hand, focuses on detecting patterns, making decisions, honing analytics, classifying data, and detecting fraud. Generative AI and traditional AI are not mutually exclusive, but complementary. While generative AI presents exciting opportunities for enterprises to overcome challenges and unlock new possibilities, traditional AI models are still able to address many use cases and tend to be less compute intensive.
Play video starting at :10:35 and follow transcript10:35
This chart shows the customization spectrum that different enterprise customers would need based on their generative AI use cases. The largest category on the left pertains to generative AI as a service, likely the one familiar to many through experiences with chat GPT. Here you input a prompt and receive a corresponding response. If the response is unsatisfactory, you can modify the prompt to obtain a new one. While the model remains constant, it receives input, refinement, or reinforcement to assist in delivering a more accurate answer. The next level, moderate customization, is where enterprises will add additional parameters to the existing pre trade model and slightly tune it. This moderate customization still requires infrastructure, expertise, and investment.
Play video starting at :11:21 and follow transcript11:21
The next one is extensive customization. Here's where customers are building their own foundational models. They require extensive fine-tuning and even more investment in infrastructure and AI expertise.
Play video starting at :11:36 and follow transcript11:36
Building foundation models from scratch requires a lot of data and compute resources, making it a very costly process. It's often large technology companies, research institutions, and well funded startups that have the resources and expertise to undertake such projects. Customizing a pre-trained model, on the other hand, requires less data resources and expertise. It involves feeding the model with domain-specific data tasks and adjusting the model parameters accordingly. It's less resource intensive, but also requires some knowledge of the model capabilities, the data format, and the evaluation metrics. As a result, many organizations choose the more cost effective approach of leveraging existing foundational models like those offered by AI research companies, and fine tune them to their specific needs.
Play video starting at :12:26 and follow transcript12:26
While generative AI shows tremendous promise for delivering business value, companies must also make substantial investments to build custom LLMs to meet their needs. Building custom LLMs requires massive amounts of training data. To get these models to understand, predict, and generate human like text, we need to feed them with a substantial corpus of diverse and high quality data. This presents a challenge in terms of not only data collection, but also its curation, storage, and management. The sheer scale of computations required for training these models is immense. It demands a robust, large-scale computing infrastructure, which is expensive.
Play video starting at :13:5 and follow transcript13:05
Implementing LLMs requires more than just the right hardware it also necessitates the right software. Organizations need tools that address both training and inference challenges. From algorithm development to accelerating inference on a distributed infrastructure, LLMs are complex and sophisticated. They require a deep understanding of AI, machine learning, and data science principles. Building and fine tuning these models require teams with a high degree of technical expertise in these areas, which can be difficult to find and retain.
Play video starting at :13:39 and follow transcript13:39
Embarking on the journey of generative AI involves some key steps. In essence, deploying generative AI in an organization is not just about the technology, but also about aligning it with business goals, team capabilities, data strategies, infrastructure readiness, and a commitment to responsible AI identify the business opportunity we must focus on use cases with substantial business impact and ones that can be enhanced by our unique data. These opportunities form the bedrock of our generative AI strategy. Build out domain and AI teams. This involves identifying our internal resources and coupling them with AI expertise from partners and application providers. Forming an interdisciplinary team that understands both our business and the AI landscape.
Play video starting at :14:28 and follow transcript14:28
Analyze data for training and customization. This is where we acquire, refine, and protect our data in order to build data intensive foundation models or customize existing ones. Invest in accelerated infrastructure. This includes assessing our current infrastructure, architecture, and operating model while carefully considering associated costs and energy consumption. The right infrastructure will enable an efficient and effective deployment of our AI solutions.
Play video starting at :14:57 and follow transcript14:57
Develop a plan for responsible AI. This means leveraging tools and best practices to ensure that our AI models and applications uphold ethical standards and operate responsibly. In essence, deploying generative AI in an organization is not just about the technology, but also about aligning it with business goals, team capabilities, data strategies, infrastructure readiness, and a commitment to responsible AI.
Play video starting at :15:25 and follow transcript15:25
Let's look at some of the steps involved in building generative AI applications. From data preparation to deploying an LLM into production.
Play video starting at :15:33 and follow transcript15:33
The data acquisition phase involves collecting and preparing the data that'll be used to train and fine-tune the LLM. The data can come from various sources such as public datasets, web scraping, user generated content, or proprietary data. It's important that the data is diverse and representative of the target domain.
Play video starting at :15:54 and follow transcript15:54
Once there is enough data gathered comes data curation. This phase involves cleaning, filtering, and organizing the data that'll be used to train and fine-tune the LLM.
Play video starting at :16:5 and follow transcript16:05
The pre-training phase of an LLM involves exposing the model to a vast corpus of text data to facilitate the learning of language patterns, relationships, and representations. This phase typically incorporates a foundational model as the starting point.
Play video starting at :16:20 and follow transcript16:20
Customization allows the adaptation of a generic model to the specific requirements of a given task or domain, thereby improving its accuracy, efficiency, and effectiveness. Model evaluation is the process of assessing the performance and effectiveness of a machine learning model. It involves measuring how well the model is learned from the training data and how accurately it can make predictions on unseen or new data.
Play video starting at :16:45 and follow transcript16:45
After a model has been trained on a dataset, it is deployed for inference, where it processes input data and produces output, such as classifications, predictions, or recommendations, depending on the specific task it was trained for.
Play video starting at :16:59 and follow transcript16:59
Adding guardrails to an LLM is crucial for fostering responsible AI practices and mitigating the risks associated with the misuse or misinterpretation of the generated text. It helps ensure ethical, safe, and responsible use of the model. The NVIDIA generative AI platform is built on robust hardware, versatile software, and high quality enterprise grade support. This combination allows NVIDIA to provide a fully production-ready generative AI solution to empower enterprises to develop custom large language models for diverse applications such as language processing, multimodal use cases, healthcare, life sciences, and visual content creation. At the pinnacle of this platform, NVIDIA has developed AI foundations, a set of tools and services designed to advance enterprise-level generative AI. These tools allow for customization across various use cases from text-based applications through NVIDIA NeMo, visual content creation with NVIDIA Picasso, and biological computations using NVIDIA BioNeMo. These services are layered atop the NVIDIA AI Enterprise, a software suite that streamlines the development and deployment of generative AI, computer vision and speech AI, allowing organizations to focus more on extracting valuable insights and less on maintenance and tuning. At the base of this technological pyramid lies NVIDIA's accelerated compute infrastructure, which is flexible and versatile. It can operate anywhere, be it on cloud platforms or on premises.
Play video starting at :18:33 and follow transcript18:33
Well done on completing this unit. Now that youve finished this unit, you should be able explain what generative AI is and how the technology works. Discuss the generative AI market trends and the challenges in this space with your customers. Describe the steps required for enterprises to unlock new opportunities for their business.
Play video starting at :18:55 and follow transcript18:55
Youve reached the end of the third unit in the AI essentials from concept to deployment course. In the next unit, we'll explore the acceleration of AI through gpus. Thank you for dedicating your time and attention.