# Lesson 9 - Storage for AI

0:07
Welcome, in this unit, we'll discuss storage considerations for AI topics covered in this unit. Storage requirements for AI workloads storage file system types Nvidia validated storage partners storage considerations summary by the end of this unit, you should be able identify the storage requirements necessary for AI workloads, explain the key concepts of storage file systems, and apply them in relevant scenarios. Comprehend the benefits of using validated storage partners in an AI data center. Summarize storage considerations for AI workloads.
Play video starting at ::50 and follow transcript0:50
Deep learning has become relevant in today's business environment because of the availability of fast computing and massive amounts of data. Model accuracy is often correlated with model complexity. In other words, as model complexity increases, it can better characterize the dataset, leading to increased accuracy. However, more complex models often require more data. For image classification tasks, training datasets can consist of millions or even billions of images. In autonomous driving, a single camera at 1080 pixels resolution can capture half a terabyte of data in 30 minutes. For natural language processing, billions of records are created every day through email, texts, tweets, and others. Most of this data is going to be stored somewhere, and the storage needs to be fast, flexible, and scalable so that it can be read and reused by many applications. Data must be stored in such a way that it can be effectively used and easily and quickly recalled by all users authorized to do so. Storage performance is often characterized in input/output operations per second, or IOPS, as well as bandwidth and metadata operations. IOPS and bandwidth both represent how quickly data can move between the storage system and the servers. Metadata operations are those related to finding, querying, and manipulating the stored data and its structure. Ideally, the data should be visible across the IT infrastructure, which simplifies the use and management of the data throughout the enterprise.
Play video starting at :2:24 and follow transcript2:24
It must be labeled in such a way that it is easy to understand what it is and from where it came. Storage mechanism needs to provide methods to know if the data remains the same as when it was written and provides resiliency so that in the event of an eventual failure, the data can still be recalled or reconstructed into its original state.
Play video starting at :2:45 and follow transcript2:45
However, to build storage systems that are resilient, robust, fault tolerant, performant, and that provide a shared view is difficult. There are many solutions today, each with their strengths, but it is important to understand the end users needs to match those correctly. When deciding on a storage solution for an environment, the full lifecycle of the data should be considered. The following questions should be asked. How will the data be written? How will it be read? How often will it be accessed? Is the storage able to provide data fast enough? Who needs to access it? What should be done when there are system failures? Are there any potential concerns about privacy of the data? When should the data be retired?
Play video starting at :3:35 and follow transcript3:35
Understanding the answers to these questions will help select the most appropriate solution. Now let's review some of the storage file systems and their usage in AI data centers.
Play video starting at :3:47 and follow transcript3:47
There are many different storage systems available today that meet a variety of needs. Most servers have a local storage system. Local storage systems are fast, can provide strong performance, and are relatively simple when compared to network and shared storage systems. However, they are not shared for multiple applications to work on the same data, that data would have to be duplicated across those systems that want access.
Play video starting at :4:13 and follow transcript4:13
Network file systems provide a local like view of data to a group of servers. This is often accomplished using open standards based protocols to allow access by servers across different operating systems. Parallel and distributed file systems share data across a group of servers and scale out to the group in both performance and capacity as needed. Often, parallel file system clients rely on custom methods to provide a local like view of the data. Depending on the type and scale of a distributed file system, it can offer the highest read and write speeds over other storage systems by far. Object storage systems provide ways to scale storage systems massively in capacity. They do this by providing APIs for accessing the data instead of providing a local like view of the data as other systems. However, object storage is not a standard, so applications must be rewritten to directly access the data.
Play video starting at :5:11 and follow transcript5:11
There are other methods of storage for large amounts of data, including SQL, NoSQL, and SQL like databases. While these provide unique performance characteristics and access methods to store and retrieve records of data, they are not as general as the other file system types discussed. The most common shared file systems are based on the network file system protocol, or NFS, which is a standard protocol. NFS was developed by Sun Microsystems in 1984 to provide a way to share data across multiple servers. Files are stored in blocks, similarly to how they're stored on a local file system. On the servers, the network file system appears to be local. Data on NFS storage is accessed via the portable operating system interface, or POSIX, which defines the behavior for operations such as open, read, seek, and close.
Play video starting at :6:6 and follow transcript6:06
NFS is a reliable solution with decent read and write performance and a simple interface. NFS appliances often have many mature features to improve usability, resiliency, and manageability of the system, including snapshots, data replication, and performance profiling tools. Nvidia storage partners that provide network file systems include NetApp, Pure Storage, and Dell EMC.
Play video starting at :6:33 and follow transcript6:33
Parallel and distributed file systems are designed to scale out in both capacity and performance by allowing the spreading of storage across multiple storage units that are all connected with a high speed network. Parallel file systems divide files into small chunks, say 1 mb in size, and spread those chunks across different storage devices. These file systems also use the POSIX standard to present data similarly to a local file system to the clients, but the file system client is unique to each file system. Distributed file systems can store files on a single storage unit, but still allow for the scaling of total performance and capacity through aggregating multiple servers that can access the data. They can often provide better single threaded and multithreaded performance when trying to maximize single node or multiple node aggregate performance. Custom alternatives to the NFS clients are used to maximize performance and support alternate communication networks such as Infiniband. Nvidia's storage partners providing parallel and distributed solutions include Data Direct Networks, IBM, and Weka IO.
Play video starting at :7:45 and follow transcript7:45
Object storage systems are designed to provide shared access to data in a more simplified manner than the other file systems discussed. They are designed to easily scale to petabytes and beyond. Even exabyte storage pools can be created. Object storage systems have no directory structure. Files are stored as blobs or buckets and referred with keys. These are key value pairs where a key can point to an entire file. Data resiliency is provided through data replication. The standard method of access is via a representational state transfer, or REST API. These rest APIs sit on top of specific data access protocols so that each object storage pool is accessed differently than the others. Object storage systems are traditionally used for the largest cloud repositories, whether public or private. They are used to retrieve data to a local network or parallel file system. Examples of object storage systems include Amazon's Simple Storage Service, Google Cloud Storage, OpenStack, and Microsoft's Azure Blob Storage.
Play video starting at :8:52 and follow transcript8:52
Let's review validate storage partners and benefits using validated storage partners solutions. A validated storage partner is a company that collaborates with Nvidia to ensure compatibility between their storage products and Nvidia's data center of solutions, including DGX Superpod and DGX Basepod. Working with a validated storage partner guarantees seamless integration between storage and Nvidia hardware in data centers. Customers can trust that their storage will function optimally with Nvidia systems, enabling them to leverage the full performance and capabilities of their Nvidia hardware.
Play video starting at :9:31 and follow transcript9:31
Validated storage partners work hand in hand with Nvidia, fine tuning their products to optimize performance with Nvidia hardware. This meticulous optimization process can unlock substantial performance improvements. The benefits don't stop at performance. These products are put through a stringent testing process, ensuring their reliability and capability to withstand the demands of intense data center workloads. Nvidia validated partners offer a comprehensive range of products designed to scale and meet the needs of large scale data center deployments. This scalability ensures that as the needs grow, the solutions can grow accordingly. In today's digital age, security is paramount. Nvidia validated partners understand this and offer a variety of security features designed to safeguard data from unauthorized access.
Play video starting at :10:22 and follow transcript10:22
The validated partners can help reduce costs. They provide optimized solutions tailored to meet the specific needs of data center deployments, eliminating the need for expensive customizations or upgrades.
Play video starting at :10:41 and follow transcript10:41
AI applications need a very large amount of data storage. While the data is primarily read by the AI applications, write is also an important part of the overall storage solution. All the pieces of the storage hierarchy, local file systems that can be used as data cache, network file systems, parallel, distributed file systems, and object storage, have their strengths. It is not a simple task to group these different technologies into one storage bucket. Often the traditional network file systems use a scale out approach which is like the distributed file systems. Some file systems provide object storage access along with the local like view of data. Parallel and distributed file systems often provide NFS support for additional compatibility. As storage for a particular purpose is evaluated, one family of technologies shouldn't necessarily be discounted because of an assumption of a missing feature.
Play video starting at :11:35 and follow transcript11:35
Many of the file systems mentioned earlier can be combined in a multi tiered storage hierarchy to offer the best performance, usability and scalability, with the faster tiers being closer to the user and the slower data lakes serving as data archive.
Play video starting at :11:52 and follow transcript11:52
To optimize storage and its access, it is helpful to understand how data is accessed during the DL training process. Data records are repeatedly accessed in random order, so it is beneficial when the storage system can handle randomly accessed files quickly and efficiently. This can put pressure on the file system's metadata performance. While the first access of data may be slow, the subsequent accesses are going to control the overall DL training performance. For this reason, it is best when the data reads can be cached locally, either in ram or on local disk. In addition to reads, writes become increasingly important as models get larger in size. For very large models, write performance should be part of the consideration. When many models are trained at the same time, storage needs are amplified. Generally, choosing a storage solution that offers fast read I o along with data caching many times offers the overall best performance for most AI workloads, data is a fundamental asset to any business and the most important asset in an AI environment. Thus, accessibility and manageability of that data is critical. There are many very good networks file systems, parallel file systems, and object storage technologies that can meet the rigorous demands of an AI data center. It is important to understand the benefits of each technology so they can be matched to the user and data center needs. There are many ways to measure performance of file systems, but the key performance metric for DL training is read and reread. Performance, the rate at which data can be accessed, is often correlated to the distance from the GPU, the closer the better. Therefore, using local system resources such as ram and local disk to cache data can increase training performance while also reducing stress on a shared storage system, preventing the need to over provision. When model sizes increase, write IO will also become more important. You cannot focus on the storage needs of training a single model. You will usually train multiple models at the same time, amplifying the storage needs Nvidia has many partners providing best of breed storage technologies that are fully integrated, tested, and ready to deploy and that reduce time to deployment and minimize system management risks. Now that you've completed this unit, you should be able to identify the storage requirements necessary for AI workloads. Explain the key concepts of storage file systems, and apply them in relevant scenarios. Comprehend the benefits of using validated storage partners in AI data center. Summarize storage considerations for AI workloads.
Play video starting at :14:36 and follow transcript14:36
Great progress. Don't stop here, continue the journey with unit 10, energy efficient computing.