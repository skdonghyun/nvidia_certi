# Lesson 7.4 - Introduction to DPUs

0:01
Now, that we have a solid understanding of GPU and CPU requirements and implementations in an AI data center, let's explore the third pillar of a data center, the DPU.
Play video starting at ::12 and follow transcript0:12
DPUs are designed to meet the infrastructure requirements that modern data centers must offer for today's cloud computing and AI workloads providing a secure and accelerated infrastructure. The best definition of the DPUs mission is to offload, accelerate, and isolate infrastructure workloads. It offloads by taking over infrastructure tasks from the server CPU, so more CPU power can be used to run applications. DPUs run infrastructure functions more quickly than the CPU can using hardware acceleration in the DPU silicone, therefore accelerating network traffic and improving applications performance.
Play video starting at ::52 and follow transcript0:52
DPUs offer isolation by moving critical data plane and control plane functions to a separate domain on the DPU to relieve the server CPU from work and protect the functions in case the CPU or its software is compromised.
Play video starting at :1:8 and follow transcript1:08
The Data Processing Unit, or DPU, is a data center infrastructure on a chip that enables organizations to build software defined hardware accelerated IT infrastructure. Running infrastructure services on the host CPU steals precious CPU cores, which impacts application performance and reduces efficiency, sometimes severely. The role of the DPU is to offload and isolate the infrastructure services from the host CPU and accelerate them in hardware by leveraging purpose built hardware accelerators, freeing up the host CPU for money making applications. And improving data center performance, efficiency, scalability, and security.
Play video starting at :1:48 and follow transcript1:48
A DPU has several specialized accelerators for networking, security, and storage. These accelerators are designed to execute these tasks much more efficiently than the CPU cores, allowing you to process greater amounts of data more quickly and often using significantly less power. It can also run compute heavy tasks in environments where the physical footprint is limited, like in far edge applications.
Play video starting at :2:12 and follow transcript2:12
The NVIDIA BlueField-3 Data Processing Unit, or DPU, is the third generation infrastructure compute platform that enables organizations to build software defined hardware accelerated IT infrastructures from cloud to core data center to edge. With 400 gigabits per second ethernet, or NDR 400 gigabits per second InfiniBand network connectivity, Bluefield-3 DPU offloads, accelerates and isolates software defined networking, storage, security, and management functions in ways that profoundly improve data center performance, efficiency, and security.
Play video starting at :2:49 and follow transcript2:49
BlueField DPUs provide a secure and accelerated infrastructure by offloading, accelerating, and isolating a broad range of advanced networking, storage, and security services. From cloud to core to edge, it increases efficiency and performance. Let's take a look at some prominent use cases for NVIDIA BlueField DPUs. The world's largest Cloud Service Providers, or CSPs, have adopted the DPU technology to optimize the data center infrastructure stack for incredible efficiency and scalability. BlueField is used in bare metal virtualized cloud data centers and more recently also in Kubernetes clusters, often running on a bare metal infrastructure. BlueField DPUs enable a secure infrastructure in bare metal clouds. For cybersecurity, we see BlueField used in Next Generation Firewalls, NGFW, micro segmentation, and all sorts of security applications. Enabling a zero trust security everywhere architecture where security goes beyond the data center perimeter to the edge of every server. HPC and AI, telco enterprise storage, and CDN content delivery networks are areas where BlueField adds much value in accelerated performance, new functionality, and more. By supporting NVME Over Fabrics, NVMEOF, GPU direct storage, data integrity, decompression and deduplication. BlueField provides high performance storage access for remote storage that rivals direct attached storage. Finally, BlueField reduces CPU cycles in video streaming by offloading and accelerating video streaming to the DPU.
Play video starting at :4:32 and follow transcript4:32
NVIDIA DOCA is the open cloud SDK and acceleration framework for BlueField DPUs. By leveraging industry standard APIs, DOCA unlocks data center innovation by enabling the rapid creation of applications and services for BlueField DPUs. It supports BlueField-3 for empowering thousands of developers, simplifying the development for networking, storage, and accelerating infrastructure services in the cloud.
Play video starting at :5: and follow transcript5:00
Now, that you have an understanding of the three pillars of the data center, GPU, CPU, and DPU, let's review the NVIDIA- certified servers that provide an end-to-end platform for accelerated computing.
Play video starting at :5:12 and follow transcript5:12
An NVIDIA certified system brings together NVIDIA GPUs and NVIDIA networking onto systems from leading vendors. It conforms to NVIDIA's design best practices and has passed a set of certification tests that validate the best system configurations for performance, manageability, scalability, and security. With NVIDIA-certified systems, enterprises can confidently choose performance optimized hardware solutions backed by enterprise grade support to securely and optimally run their accelerated computing workloads, both in smaller configurations and at scale. NVIDIA certified servers help to secure workflows by protecting data at the platform, network and application layers. Whether deployed in a data center or at the edge, laptops or desktops, customers can be assured that they don't have to compromise on security features when running accelerated applications.
Play video starting at :6:7 and follow transcript6:07
Certified servers bring together a whole set of technologies and server configurations that have been validated for the most optimal functionality. Depending on the choice of GPU and network adapter, workloads can benefit from numerous capabilities for performance, security, and scalability.
Play video starting at :6:23 and follow transcript6:23
GPUs provide record setting acceleration of many algorithms in machine learning, deep learning, and data analytics. In addition to fast video processing and rendering, high speed interconnects allow data to be moved quickly to servers and directly to GPUs for faster processing.
Play video starting at :6:41 and follow transcript6:41
Network encryption offload for TL's and IPsec provides security for data in motion without compromising throughput as key management and secure boot features provide host level security. Accelerated data transfer between GPUs and servers unlocks efficient multi-node processing for the biggest tasks such as large AI model training. On the other extreme, multi-instance GPUs which allow a single GPU to be split into multiple independent GPU instances, allow for dynamically scaling out within a host, enabling flexible utilization.
Play video starting at :7:17 and follow transcript7:17
Now, that you've completed this unit, you should be able indicate the key components and features of the NVIDIA data center platform. Identify the GPUs and CPUs requirements for AI data centers, the different products available, and their intended use cases. Understand the purpose and capabilities of multi-GPU systems. Describe the multi-node GPU interconnect technology. Determine the role of DPUs and DOCA in an AI data center. Evaluate the benefits of using NVIDIA-certified systems.
Play video starting at :7:51 and follow transcript7:51
Continue the journey by taking the next unit networking for AI.