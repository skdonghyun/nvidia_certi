## **Lesson 7.3: 멀티-GPU 시스템**

## **강의 개요**

이 단원에서는 **AI 워크로드를 스케일링하기 위한 멀티-GPU 시스템**에 대해 다룹니다. AI 데이터 센터를 위한 GPU와 CPU 솔루션을 살펴본 이전 단원에 이어, 이 단원에서는 증가하는 워크로드 수요에 대응하기 위한 스케일링 전략, 특히 **멀티-GPU 시스템**에 초점을 맞춥니다.

---

## **학습 목표**

이 단원을 마치면 다음을 수행할 수 있습니다:

- **스케일 업(Scale-Up)과 스케일 아웃(Scale-Out)의 차이점 이해**: 멀티-GPU와 멀티-노드 스케일링 옵션을 비교합니다.
- **멀티-GPU 시스템의 필요성과 구현 이해**: AI 솔루션의 복잡성이 증가함에 따라 멀티-GPU 시스템의 중요성을 이해합니다.
- **NVLink 및 NVSwitch 기술의 역할 설명**: GPU 간 고속 통신을 위한 NVIDIA의 기술을 설명합니다.
- **NVIDIA DGX H100, DGX B200, GB200 NVL 72 시스템의 특징 파악**: NVIDIA의 최신 멀티-GPU 시스템의 사양과 적용 사례를 이해합니다.

---

## **핵심 내용 분석**

### **1. AI 워크로드 스케일링 전략**

- **스케일 업(Scale-Up)**:
  - **정의**: 단일 노드에 더 많은 GPU를 추가하여 컴퓨팅 파워를 증가시킵니다.
  - **하드웨어 요구사항**: 여러 GPU를 지원하는 노드와 GPU 간 통신을 위한 고속 인터커넥트 필요.
  - **특징**:
    - GPU 간 로드 밸런싱 필요.
    - 단일 GPU의 장애가 전체 시스템에 영향을 미칠 수 있음.

- **스케일 아웃(Scale-Out)**:
  - **정의**: 시스템에 더 많은 노드를 추가하여 전체 처리 능력을 증가시킵니다.
  - **하드웨어 요구사항**: 각기 독립적인 처리 능력을 가진 여러 노드가 네트워크로 연결됨.
  - **특징**:
    - 노드 간 로드 밸런싱 필요.
    - 한 노드의 장애가 전체 시스템에 큰 영향을 미치지 않으므로 장애 허용도가 높음.

### **2. 멀티-GPU 시스템의 필요성**

- **복잡한 AI 솔루션의 증가**: 컴퓨팅 용량의 급격한 성장이 필요하며, 이를 충족하기 위해 멀티-GPU 시스템이 도입되었습니다.
- **GPU 간 통신의 중요성**: 지속적인 성능 향상을 위해 시스템 내 GPU 간 유연하고 고대역폭의 통신이 필수적입니다.
- **PCIe의 한계**: 전통적인 서버에서는 PCIe를 통해 GPU 간 통신을 하지만, 워크로드가 커짐에 따라 PCIe의 대역폭이 병목 현상을 일으킵니다.

### **3. NVIDIA의 고속 인터커넥트 기술**

- **NVLink**:
  - **도입 배경**: PCIe보다 훨씬 빠른 속도로 여러 GPU를 연결하여 GPU 간 고속 통신과 메모리 공유를 가능하게 합니다.
  - **한계점**: 모든 GPU가 서로 통신해야 하는 경우, 일부 GPU 쌍은 여전히 느린 PCIe 경로를 통해 통신해야 함.

- **NVSwitch**:
  - **기능**: NVSwitch 기술은 어떤 GPU 쌍도 병목 없이 직접 통신할 수 있게 해줍니다.
  - **구조**: 각 GPU는 NVLink 인터커넥트를 통해 모든 NVSwitch 패브릭과 통신합니다.
  - **이점**: GPU 간 최대 대역폭으로 통신할 수 있어 대규모 AI와 HPC 워크로드에서 성능을 극대화합니다.

### **4. NVIDIA DGX 시스템 소개**

#### **a. DGX H100 시스템**

- **사양**:
  - **GPU**: 8개의 H100 GPU 탑재.
  - **GPU 간 통신**: 4세대 NVLink 스위치 4개로 고속 GPU 간 연결 제공.
  - **네트워킹**: 400Gbps 대역폭의 NVIDIA ConnectX-7 네트워크 인터페이스 10개로 최대 1TB/s의 양방향 네트워크 대역폭 제공.
  - **CPU**: Intel Xeon Platinum 8480C 프로세서 2개 (총 112코어).
  - **메모리**: 2TB의 시스템 메모리.
  - **스토리지**: 30TB의 NVMe SSD 스토리지.
  - **성능**: 32페타플롭스(FP8)의 AI 연산 성능.
- **소프트웨어**:
  - **DGX OS**: AI 및 분석 워크로드 실행을 위한 턴키 솔루션으로, Ubuntu Linux의 맞춤 설치와 시스템 최적화, 추가 드라이버, 진단 및 모니터링 도구 포함.
- **물리적 사양**:
  - **크기**: 표준 19인치 랙에 맞는 8U 높이의 섀시.
  - **구성 요소**: 금색 베젤, 듀얼 팬 모듈 12개, U.2 NVMe 드라이브 8개, 전면 콘솔 보드 등 모듈식 설계로 구성.

#### **b. DGX B200 시스템**

- **최신 NVIDIA DGX 플랫폼의 새로운 모델로, 생성형 AI의 다음 단계를 정의합니다.**
- **사양**:
  - **GPU**: 8개의 NVIDIA Blackwell GPU 탑재, 5세대 NVLink로 상호 연결.
  - **GPU 메모리**: 1.4TB의 대용량 GPU 메모리.
  - **CPU**: Intel Xeon Platinum 8570 프로세서 2개.
  - **시스템 메모리**: 2TB.
  - **성능**:
    - **훈련**: 72페타플롭스(FP8).
    - **추론**: 144페타플롭스(FP4).
- **적용 사례**: 대형 언어 모델, 자연어 처리 등 가장 복잡한 AI 문제에 대한 획기적인 성능 제공.

#### **c. NVIDIA GB200 NVL 72 시스템**

- **가장 컴퓨팅 집약적인 워크로드를 위한 멀티-노드, 액체 냉각 랙 스케일 시스템입니다.**
- **사양**:
  - **CPU**: 36개의 Grace CPU.
  - **GPU**: 72개의 Blackwell GPU, 5세대 NVLink로 상호 연결.
  - **냉각**: 액체 냉각 랙을 통해 데이터 센터의 에너지 소비를 줄이고 컴퓨팅 밀도를 높입니다.
  - **성능**:
    - **컴퓨트 패브릭 대역폭**: 130TB/s.
    - **엑사플롭급 AI 슈퍼컴퓨터**: 단일 랙에서 실현.
- **적용 사례**: 트릴리언(1조) 파라미터 규모의 대형 언어 모델 훈련과 실시간 추론 제공.
