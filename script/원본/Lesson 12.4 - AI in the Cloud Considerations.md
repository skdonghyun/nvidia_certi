# Lesson 12.4 - AI in the Cloud Considerations

0:00
This subunit explores several considerations for deploying AI in the cloud. Before we dive into the considerations, it's crucial to comprehend the AI maturity model and determine where your organization stands within it. This understanding will guide your decision-making and ensure that your cloud strategy aligns with your AI capabilities and goals. Navigating the complexities of AI can be challenging, but strategic decisions and investments can help organizations gain a competitive edge and progress along the AI growth curve. The AI maturity model is a framework for assessing the level of maturity of an organization's AI capabilities. The model consists of five stages, each representing a higher level of maturity and capability. Awareness. At this stage, organizations are just beginning to explore AI and understand its potential benefits and challenges. There may be some early conversations and experiments with AI, but no formal AI strategy or investments have been made. Active AI experimentation. Organizations at this stage are actively experimenting with AI in a data science context, such as using machine learning algorithms to analyze data and identify patterns. They may have a small team of data scientists and engineers working on AI projects, but AI is not yet pervasive throughout the enterprise. Operational AI in production. At this stage, organizations have successfully implemented AI in production environments and are using it to automate processes and improve decision-making. They've established best practices for AI development and deployment and have access to a wide range of AI technologies and experts. Systemic AI. Organizations at this stage have fully integrated AI into their digital strategy and are using it to drive innovation and growth. AI is considered a core component for all new digital projects, and the organization has a well-defined AI roadmap and strategy. Transformational AI. At this stage, AI is deeply ingrained in the organization's DNA and is a key driver of business strategy and operations. The organization has a strong AI culture and has fully integrated AI into all aspects of its business from product development to customer service. By assessing your organization's current level of AI maturity using this model, you can identify areas for improvement and develop a roadmap for advancing your AI capabilities to drive business growth and success. Let's explore some considerations, starting with AI on-prem.
Play video starting at :2:44 and follow transcript2:44
Before delving into these factors, it's helpful to determine your current starting point and operational mindset, as this will serve as a foundation for your exploration and decision-making process. On one end of the infrastructure spectrum are those who start on-prem. An organization can operate on-prem and still be in the very early stages of the AI maturity model. Having your own gear doesn't mean you've reached critical mass of model prototyping volume or have built a production workflow for AI apps or of consolidated development silos onto shared infrastructure. It simply means that you looked at the ongoing cost of reserved cloud instances versus the fixed cost of a system and decided the TCO or total cost of ownership was in favor of ownership. This is the CapEx versus OpEx consideration. It might also mean that keeping your data within the four walls of your data center is paramount. By maintaining control over your data on-premises, you can ensure data sovereignty and adhere to regulatory requirements, providing a critical layer of security and compliance. By deploying AI on-premises, you can quickly spin up and down resources to accelerate your AI development cycles, achieving the fastest iteration speed possible. With on-premises AI, you can enjoy predictable costs that scale linearly with your usage, allowing you to better plan and budget for your AI initiatives. Exploring these considerations can help you prepare and design a path to your ideal AI solution. Now let's turn our attention to the alternative approach of starting AI in the cloud and explore the considerations of that path. On the other end of the infrastructure spectrum are those who start in the cloud. For many organizations, a cloud-first or cloud-only approach is the starting point for their AI infrastructure strategy, and this guiding principle often influences the decisions they make about how to deploy and manage their AI systems. By leveraging cloud-based AI services, you can elastically scale your AI resources to meet changing temporal needs without being limited by fixed on-premises infrastructure. There are minimal barriers to entry. All cloud providers offer in NVIDIA GPU instances, and you can easily turn them on and off like a faucet. This elasticity is ideal for organizations in the early stages of their AI journey. They're still experimenting with productive AI applications and don't yet have ongoing resource demands. Training runs are short, since their models are small, and their datasets are limited. However, as they mature in their AI capabilities, they'll begin to see AI as essential and require more advanced and specialized resources to support their growing models and datasets. This will lead to an increase in cloud operating expenses. We know that many customers are already experiencing this inflection point, so it's important to consider the trade-offs, which we'll address next. Which route should you take, cloud or on-prem? Imagine an AI workflow where data is stored locally while compute resources are based in the cloud, you could experience a performance boost of 2-6 times with specialized AI infrastructure compared to cloud-based solutions. Additionally, every 60 miles of distance between the data and the cloud can result in a one millisecond increase in latency, driving up the cost of data gravity. Furthermore, 62% of IT decision makers at large enterprises believe that their on-premises security is stronger than cloud security. As you navigate your AI journey, you face the question of whether to use cloud or on-premises infrastructure for your AI workloads. However, it's not a one or the other scenario. A hybrid approach is necessary, leveraging both cloud and on-premises solutions depending on where you are in your AI journey. Initially, you may start with a modest experimental approach in the cloud, using cloud-hosted training capacity to quickly get started with minimal resources and budget. The cloud is a great place to build skills and experiment with AI. However, as you scale and your datasets grow larger, an inflection point is reached, and you may need to transition to on-premises solutions to support production scale AI applications. This is when the impact of data gravity becomes more pronounced, and developers spend more time grooming each training run to avoid failure. This slows down iteration speed and can stifle innovation. To overcome these challenges, you need to adopt a hybrid approach that leverages the strengths of both cloud and on-premises solutions. By doing so, you can optimize your AI workflow, reduce costs, and improve the speed and efficiency of your AI development processes. Let's explore this need for flexibility further. Here's a common use case that leverages a hybrid approach to AI deployment. Training, customization, and optimization take place on-premises, utilizing local resources. This approach offers several advantages, including full control over the training environment. By training models on-premises, organizations can maintain complete control over the training process, including the hardware, software, and data used. Maintaining data sensitivity and compliance. On-premises training allows organizations to keep their data within their own networks, ensuring that it remains secure and compliant with regulations. Data gravity alignment. By training models on-premises and then deploying them in the cloud, organizations can align their data gravity with their AI workloads, reducing latency and improving performance. These models are then loaded into production inference within the cloud, leveraging the scalability and flexibility of cloud infrastructure to efficiently handle varying workloads during the inference phase. This hybrid approach allows organizations to take advantage of the strengths of both on-premises and cloud-based infrastructure while minimizing the weakness and risks associated with each. Let's close out this topic with a recap of the key considerations.
Play video starting at :8:54 and follow transcript8:54
Customer needs around the deployment and development of AI are evolving, influenced by factors such as data location, application specificities, and enterprise IT strategy. Some enterprises adopt a cloud-first approach, while others prefer an "own the base, rent the spike" strategy or a multi-cloud hybrid model. AI workloads may need to remain on-premises or in specific geographic locations within the public cloud due to constraints related to real-time performance, data residency, or data sovereignty requirements. As enterprises diversify their IT strategies, there's a growing need for AI platforms that offer flexibility in developing and deploying AI applications across various environments. Data locality. Moving compute closer to where the data resides to minimize network congestion and improve application performance. Data sovereignty. Adhering to country-specific requirements governing where data originating in a geographic location must be stored and processed. Hybrid IT strategies. Growth in hybrid cloud, multi-cloud approaches to leverage best-of-breed solutions for AI POCs, training, and deployment at scale. Real-time performance. Supporting applications that need to respond in real-time or provide real-time analytics and insights based on sensor-generated data. Great momentum. Don't stop now. Next up is Subunit 12.5, which details the supported cloud service providers and their consumption models.