# Lesson 12.6 - NVIDIA Solutions in the Cloud

0:00
In this final subunit, we'll explore NVIDIA's AI cloud based solutions and how they can help you harness the power of AI in the cloud.
Play video starting at ::10 and follow transcript0:10
NVIDIA AI platform is designed to address the challenges of enterprise AI and meet customers where they're at. With this full stack platform, enterprises can use the power of AI to deliver business results irrespective of where they are in their AI adoption journey. NVIDIA AI platform is cloud native, customers can use it to develop and deploy AI enabled applications anywhere from any public cloud to enterprise data centers to edge locations, without being locked into any one cloud or deployment option. Using this stack as a reference, we'll look at what each layer of the NVIDIA AI platform looks like in the public cloud and the different ways to drive the consumption of NVIDIA technology for cloud customers. Let's start at the bottom of the stack and work our way up. The foundation of the NVIDIA AI platform is accelerated infrastructure, which in the context of cloud computing refers to virtual machine instances equipped with NVIDIA GPU's. Additionally, some cloud service providers, or CSP's, incorporate NVIDIA networking technologies to achieve at scale performance.
Play video starting at :1:17 and follow transcript1:17
The NVIDIA virtual machine images, or VMIs, available through CSP marketplaces, underlie application software, whether sourced from the NGC catalog or generic AI software. NVIDIA VMIs provide an operating system environment for running NVIDIA GPU accelerated software in the cloud. These VM images are built on top of Ubuntu OS and are packaged with core dependencies. VMIs provide a GPU optimized development environment for your GPU accelerated application on a cloud service provider's infrastructure. VMIs are essentially the operating system for cloud VMs, VMIs sit on top of cloud instance types. A cloud instance type is a predefined virtual server configuration provided by a cloud service provider specifying computing resources like CPU, memory, storage, and networking for virtual machines. Users choose instance types based on workload, scalability and budget, with options like general purpose, compute optimized, memory optimized, and GPU instances.
Play video starting at :2:22 and follow transcript2:22
Now that you've grasped the cloud accelerated infrastructure layer, let's ascend the NVIDIA AI stack to touch briefly on NVIDIA AI Enterprise. Let's talk about the next layer, which is the API platform software layer, NVIDIA AI Enterprise is the AI platform software layer. Software is what enables enterprise AI applications to leverage the power of the underlying accelerated infrastructure. Application performance has a direct tie in to operational costs in the cloud, which is why you want to make sure you're always getting the most of your compute resources. With NVIDIA optimized and enterprise supported software, customers can get both the best performance from the accelerated infrastructure and accelerate time to solution.
Play video starting at :3:9 and follow transcript3:09
While we covered NVIDIA AI enterprise in unit five, it's essential to emphasize its deployability in the cloud. In fact, NVIDIA AI enterprise represents a secure, end to end and cloud native AI software platform specifically designed for production AI workloads. The solution is available across multiple deployment environments, including public, hybrid and multi clouds. Another key component of the AI platform in the cloud is NGC, let's explore that next.
Play video starting at :3:40 and follow transcript3:40
Let's connect the dots between NVIDIA AI Enterprise and the NVIDIA NGC catalog. NVIDIA NGC serves as a central hub for all NVIDIA services, software and support, providing customers with a one stop shop for our AI offerings. With a subscription or license to NVIDIA AI Enterprise, customers gain access to the enterprise catalog hosted on NGC, which includes AI workflows and new AI services. However, free software access through NGC does not provide the same level of benefits as NVIDIA AI enterprise, such as enterprise support, SLA's, access to NVIDIA AI experts, and exclusive enterprise product differentiators. Let's continue up the NVIDIA AI platform stack to AI services.
Play video starting at :4:26 and follow transcript4:26
The topmost layer of the NVIDIA AI platform is the AI services layer, this is the newest addition to the NVIDIA cloud portfolio. It is the highest level of abstraction at which customers can engage with our platform. It brings to bear value of the entire NVIDIA AI platform to the end customer as an NVIDIA managed service. Let's start with NVIDIA DGX cloud and work our way up the AI services.
Play video starting at :4:52 and follow transcript4:52
Consider the following with traditional AI development on traditional clouds, DIY tools and open source software is used to patch together a solution. Inconsistent access to multinode scaling across regions, searching through community forums, and voluntary contributions to find answers to your questions if you're lucky. Escalating costs, add on fees for reserved instances, storage and data egress, NVIDIA DGX cloud is a multinode AI training as a service solution for enterprise AI. When a model is too large to fit into a GPU's memory, then multinode training is advantageous, within a single service that's offered at an all in one monthly price. It brings together the NVIDIA base command platform, NVIDIA AI enterprise software, NVIDIA DGX infrastructure combined with access to NVIDIA AI expertise and support. Customers can just open a browser to get started without having to procure, set up and manage an AI supercomputer on their own. As a service, DGX Cloud is hosted across multiple public clouds like Oracle cloud infrastructure, Microsoft Azure and Google Cloud.
Play video starting at :6:3 and follow transcript6:03
Having gained a fundamental comprehension of the DGX cloud solution, let us delve into the realm of NVIDIA AI foundations.
Play video starting at :6:15 and follow transcript6:15
NVIDIA AI Foundations is another suite of NVIDIA managed cloud services, powered by the NVIDIA DGX cloud. NVIDIA AI Foundations is a set of cloud services for enterprises to build and run custom generative AI by leveraging state of the art foundation models for text language, visual media and biology. There are currently two collections that are part of the NVIDIA AI foundations.
Play video starting at :6:40 and follow transcript6:40
NVIDIA introduced the Nemotron-3-8B enterprise ready family of models which has been trained using responsibly sourced data. These models deliver results comparable to larger models but with a reduced inference cost. Ideal for global enterprises, these models support over 50 spoken and 35 coding languages. They find application in various scenarios such as chat and Q&A applications across diverse industries including healthcare, telecommunications and financial services. Community models optimized by NVIDIA for both throughput and latency using tensorrt LLM, ensuring the utmost performance efficiency. Achieving a 2x higher inference on LLAMA 2 with TRT LLM, these models include LLAMA 2, Mistral, stable diffusion and code llama. Streamlined for customization all models are converted to the ,Neemo format. This allows developers to make the most of Neemo's data preparation guardrails and advanced customization techniques, facilitating the fine tuning of these foundational models with proprietary data on DGX cloud.
Play video starting at :7:50 and follow transcript7:50
Explore the models using the fully accelerated NVIDIA AI stack, test the models directly from your browser through a GUI or app without the need for additional setup. Seamlessly connect enterprise applications to NVIDIA hosted API endpoints to assess the full potential of the models in real world applications. These models can be found in the NVIDIA NGC catalog, are accessible on several CSP's, and are also featured on the hugging face website.
Play video starting at :8:18 and follow transcript8:18
Let's delve into the uppermost tier of the NVIDIA AI services stack known as NVIDIA AI foundry.
Play video starting at :8:26 and follow transcript8:26
An AI foundry is a new kind of service for creating custom generative AI models. The service should provide pioneering state of the art pre trained models, utilities for effortless customization of models with proprietary data, cloud native infrastructure with accelerated capabilities.
Play video starting at :8:47 and follow transcript8:47
These elements come together to enable the creation of customized enterprise grade models at scale.
Play video starting at :8:54 and follow transcript8:54
The NVIDIA AI foundry service gives enterprises an end to end solution for creating custom generative AI models, it encapsulates three elements. NVIDIA AI foundation models, which we covered earlier, encompasses state of the art, pre trained models from NVIDIA, along with NVIDIA optimized community foundation models, the models are hosted in CSP's model catalog. The NVIDIA Nemo framework provides tools for fine tuning models with enterprise grade runtime, incorporating guardrails, optimizations, and advanced customization techniques. NVIDIA DGX Cloud, which we covered earlier, is a serverless AI training as a service platform for enterprise developers that runs on various hyperscalers and is first being introduced with Microsoft Azure. Users can rent NVIDIA DGX cloud, now available on Azure, and it comes with NVIDIA AI enterprise, including Nemo to speed LLM customization.
Play video starting at :9:50 and follow transcript9:50
The output is a custom model container tuned with proprietary data, guardrails and inference runtime. Once customized, these enterprise proprietary models can be deployed virtually anywhere on accelerated computing with enterprise grade security, stability and support using NVIDIA AI enterprise. Let's end the unit with a review of the ways in which you can consume NVIDIA solutions on the cloud. In summary, the effective deployment and utilization of AI capabilities in the cloud requires a keen focus on consumption. Encompassing both the allocation of cloud resources and the optimization of associated costs, in order to unlock the full potential of AI in driving business success. NVIDIA accelerated infrastructure in the cloud is no doubt the foundational piece of making our technology available broadly to cloud customers. However, NVIDIA has come a very long way from doing just that and have built an entire full stack platform that can now be consumed in the cloud. Be it full stack consumption with the AI services provided with DGX cloud AI foundations or AI foundry AI services. Software and infrastructure consumption with NVIDIA AI enterprise software or infrastructure consumption with different layers of the NVIDIA AI platform, combined with our integrations on CSP's. Customers have a path to use and derive value from NVIDIA, even within cloud services that they may already use today.
Play video starting at :11:18 and follow transcript11:18
Now that you've completed this unit, you should be to, able explain the various ways cloud computing enhances AI deployments, describe the wide variety of AI use cases in cloud computing environments. Outline the key considerations and strategies when deploying AI in the cloud, summarize the wide variety of cloud service providers that support NVIDIA technologies and solutions. Categorize the various cloud consumption models when deciding how to consume cloud services. Evaluate NVIDIA's cloud solutions and how they can benefit your workloads. Concluding this unit marks a significant milestone in our journey through the introduction to AI in the data center course. Fantastic progress so far, as we look forward, we're set to explore unit 13, AI data center management and monitoring, see you in the next unit.