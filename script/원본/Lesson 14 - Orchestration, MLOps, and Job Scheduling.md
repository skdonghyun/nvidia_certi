# Lesson 14 - Orchestration, MLOps, and Job Scheduling
0:05
Welcome to Unit 14. In this unit, we'll be covering orchestration and job scheduling. In this course, we'll give an overview of orchestration and scheduling. We will discuss Kubernetes and machine learning operations, MLOps, and the Slurm scheduler. By the end of this unit, you should be able to describe the difference between orchestration and scheduling, describe common tools used for orchestration and scheduling, and discuss the value of MLOps tools. Let's discuss the concepts of orchestration and scheduling. Container orchestration is a way to automate the operations related to containers. Orchestration tools handle these tasks based on the needs of the environment that is being managed. Scheduling is the process of assigning workloads to available compute resources. Orchestration is container-based. Scheduling is bare-metal based, but it supports containers. Orchestration is designed for micro-services. Scheduling is designed for high-performance computing. Orchestration can scale up and down based on inferencing needs. Schedulers do not have inferencing capabilities. Orchestration requires additional meta schedulers for advanced features. Schedulers have advanced features, such as priority and preemption built-in. Orchestration manages entire workflows and processes. Scheduling handles assigning tasks and jobs to the available resources. Orchestration does load balancing to distribute traffic across containers. Scheduling determines the hosts with the right resources to run containers. Kubernetes is an open source container orchestration system for automating software deployment, scaling, and management. Google originally designed Kubernetes, but the Cloud Native Computing Foundation now maintains the project. Container-based jobs are defined. For example, a CUDA container will run the command nvidia-smi on a node that requires one GPU. The master node determines which worker node meets the job criteria and deploys the job to that worker node. The Kubernetes pod is a group of containers being run as an application on that worker node. Now we'll discuss the primary components in Kubernetes. A node is a server with a defined CPU, memory, GPU, etc., resources that Kubernetes can use. Anything that runs in OS, virtual or physical hardware. A cluster is a collection of one or more nodes that can run containerized workloads. A namespace is an isolated environment intended for multi-tenancy. Resources inside a namespace must be unique and are exclusive to a single namespace. A namespace can be allocated a resource quota. A container is a self-contained run time with the necessary dependencies. It is created from fixed images and only exists while being run. A pod is a group of one or more containers that are controlled as a single application. It encapsulates application containers, storage resources, and a unique network ID. A pod also maintains configuration on how to run the containers. Persistent volumes are the storage mounted on all containers in the pod. Data is preserved on container restarts. Pods can have multiple volumes. The volumes exist outside the scope of pod. Finally, services handle networking and other tasks like deploying pods, creating replicas of pods, and deploying copies of pods when necessary. The NVIDIA GPU Operator is an open source software that gives IT infrastructure teams everything they need to use GPUs on Kubernetes. It does this by automating the deployment and management of all NVIDIA software required to use GPUs on Kubernetes. The NVIDIA GPU Operator includes the NVIDIA Data Center GPU Manager or DCGM, which is a suite of tools for managing and monitoring NVIDIA data center GPUs in cluster environments. DCGM simplifies GPU administration in the data center, improves resource reliability and uptime, automates administrative tasks, and helps drive overall infrastructure efficiency. This allows organizations faster time to outcome and an error-free automated path to scaling their GPU-accelerated applications on Kubernetes. The NVIDIA Network Operator is a tool that can be installed on top of GPU Operator to enable further GPU capabilities with Kubernetes, including GPUDirect RDMA, remote direct memory access. The MLNX_OFED is a set of networking libraries and drivers packaged and tested by the NVIDIA networking team. MLNX_OFED supports RDMA over both InfiniBand and Ethernet interconnects. The Network Operator's goal is to install the host networking components required to enable RDMA and GPUDirect in a Kubernetes cluster. The NVIDIA peer memory driver is a client that interacts with the network drivers to provide RDMA between GPUs and host memory. The network operator works with the GPU operator to deploy networking software. In this section, we'll discuss machine learning operations or MLOps. Machine learning operations or MLOps tools are often used with Kubernetes to bring an organized methodical discipline to AI project operation and deployment, helping to ensure the AI infrastructure is well-utilized. Data preparation, versioning of data, models, and other parameters, monitoring experiments and results, deploying models for inferencing, monitoring model performance. The MLOps tools also help to improve user productivity and speed up workflows, maximize utilization of resources, and allow projects to scale. NVIDIA has several MLOps partners with tools that support a variety of MLOps capabilities. These logos represent some of these partners. To learn more about MLOps solutions and partners, visit the site linked on this slide. Slurm is a tool that is used for job scheduling. Slurm was created by SchedMD and handles scheduling of workloads. It's an open-source cluster management and job scheduling system for Linux clusters that is highly scalable and fault-tolerant. Requires no kernel modifications, schedules jobs to run on a subset of cluster resources, excellent for AI training, highly performant, closely tied to nix systems, such as Unix and Linux, and is easy to integrate with existing security mechanisms. Let's take a closer look at the Slurm system design. There are two main components. The Slurm controller is the centralized manager. It manages workloads and the available resources in the cluster. The compute nodes are responsible for running the jobs and returning the results. The user sends commands to the Slurm controller that manages sending jobs to the compute nodes. Jobs are scheduled on the compute nodes based on resources needed for the job. An optional database can be used with Slurm to track accounting records. NVIDIA recommends using containers to run jobs. Enroot and pyxis are NVIDIA tools that allow the use of containers with the Slurm scheduler. Enroot turns containers into unprivileged sandboxes. Pyxis integrates enroot with Slurm. Enroot and pyxis work together to ensure that containers do not have root access to compute nodes that they're running on. Now that you've completed this unit, you should be able to describe the difference between orchestration and scheduling, describe common tools used for orchestration and scheduling, and discuss the value of MLOps tools. Now that you've completed the learning units, you're prepared to take on the course completion quiz. Good luck.
